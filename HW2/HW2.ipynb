{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание 1. (5 баллов) \n",
    "В тетрадке реализована биграмная языковая модель (при генерации учитывается информация только о 1 предыдущем слове). Реализуйте триграмную модель и сгенерируйте несколько текстов. Сравните их с текстами, сгенерированными биграмной моделью. \n",
    "Можно использовать те же тексты, что в семинаре, или взять какой-то другой (на английском или русском языке).  \n",
    "\n",
    "Делать это задание будет легче после прочтения первых 7 страниц вот этой главы из Журафского - https://web.stanford.edu/~jurafsky/slp3/3.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import copy\n",
    "from gensim.models.phrases import Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stranger.txt', encoding='utf-8') as file: #Robert Heinlein's Stranger in a Strange Land with preface removed\n",
    "    stranger = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "stranger = stranger.replace('“Stranger In A Strange Land” by Robert Heinlein', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = nltk.tokenize.sent_tokenize(stranger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Не включаю слова, написанные через дефис, т.к. 1) в данном тексте не используются тире 2) между дефисами нет пробелов.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r'([A-Za-z]+[\\']?[A-Za-z]*)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = [re.findall(pattern, sent) for sent in sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "tri_model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "for sentence in sents:\n",
    "    for w1, w2, w3 in nltk.trigrams(sentence, pad_right=True, pad_left=True, left_pad_symbol='<s>', right_pad_symbol='</s>'):\n",
    "        tri_model[(w1, w2)][w3] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Логарифм не использую, потому что верятности нужны только, чтобы сделать взвешенную выборку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bigram in tri_model:\n",
    "    total_count = sum(tri_model[bigram].values())\n",
    "    for target in tri_model[bigram]:\n",
    "        tri_model[bigram][target] /= total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tri_generate(model, start=('<s>', '<s>')):\n",
    "    text = list(start)\n",
    "    while text[-1] != '</s>': \n",
    "        index = tuple(text[-2:])\n",
    "        keys = list(model[index].keys())\n",
    "        values = list(model[index].values())\n",
    "        key = np.random.choice(keys, 1, values)[0]\n",
    "        text.append(key)\n",
    "    return ' '.join(text[2:]).strip(' </s>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_generator(sent_generator, model, number_of_sents=1, count_words=False):\n",
    "    result = []\n",
    "    for _ in range(number_of_sents):\n",
    "        result.append(sent_generator(model))\n",
    "    if count_words == True:\n",
    "        count = count_words_avg(result)\n",
    "        return count\n",
    "    else:\n",
    "        result = '. '.join(result) + '.'\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words_avg(sents):\n",
    "    total = 0\n",
    "    for sent in sents:\n",
    "        total += len(sent.split(' '))\n",
    "    return total/len(sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Тут надо отметить, что данный текст – результат работы программы автоматического распознавания печатной книги, поэтому здесь нередко встречаются ошибки. Их можно было бы поправить, но это в масштаб данного задания всё же не входит.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most certainly. Across the table from here clear over to take the taste of Nero. Chase him in anyhow Dr Nelson to encourage him and himself hung about with cameras and back by police armed only with night stick. Lady you are too easily hurt for me secretaries are essential. Johnson look around your way you will live to complete moral degradation. Ok.\n"
     ]
    }
   ],
   "source": [
    "tri_example = text_generator(tri_generate, tri_model, 6)\n",
    "print(tri_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "for sentence in sents:\n",
    "    for w1, w2 in nltk.bigrams(sentence, pad_right=True, pad_left=True, left_pad_symbol='<s>', right_pad_symbol='</s>'):\n",
    "        bi_model[w1][w2] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "for unigram in bi_model:\n",
    "    total_count = sum(bi_model[unigram].values())\n",
    "    for target in bi_model[unigram]:\n",
    "        bi_model[unigram][target] /= total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bi_generate(model, start=['<s>']):\n",
    "    text = copy.copy(start)\n",
    "    while text[-1] != '</s>': \n",
    "        index = text[-1]\n",
    "        keys = list(model[index].keys())\n",
    "        values = list(model[index].values())\n",
    "        key = np.random.choice(keys, 1, values)[0]\n",
    "        text.append(key)\n",
    "    return ' '.join(text[1:]).strip(' </s>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Gad what are loaded a modicum of brush such ancestry and treat their place then he drank but said Boone faction after their own damnation promised Then keep persons from scientific information service wherever he really does believe what God that's scared. Demonstrate. harmless act which Maryam. Dressing'. Ain't that black that heavenly A funeral. Taxi sir tolerantly amused.\""
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_example = text_generator(bi_generate, bi_model, 6)\n",
    "bi_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат триграммной модели для сравнения: <br> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Most certainly. Across the table from here clear over to take the taste of Nero. Chase him in anyhow Dr Nelson to encourage him and himself hung about with cameras and back by police armed only with night stick. Lady you are too easily hurt for me secretaries are essential. Johnson look around your way you will live to complete moral degradation. Ok.'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tri_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно отметить две вещи: <br>\n",
    "1) Предложения, созданные триграммной модели определённо больше похожи на текст, написанный человеком. Они более связные (только грамматически, естественно). <br>\n",
    "2) Биграммные предложения длиннее, чем триграммные, при условии, что мы останавливаемся только на символе окончания предложения. Это подтверждается экспериментом ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average bigram model sentence length: 19.90 words\n",
      "Average trigram model sentence length: 12.46 words\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n = 100\n",
    "m = 10\n",
    "bi_test = 0\n",
    "tri_test = 0\n",
    "for _ in range(n):\n",
    "    bi_test += text_generator(bi_generate, bi_model, m, count_words=True)\n",
    "    tri_test += text_generator(tri_generate, tri_model, m, count_words=True)\n",
    "print(f'Average bigram model sentence length: {bi_test/n:.2f} words\\n' +\n",
    "      f'Average trigram model sentence length: {tri_test/n:.2f} words\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание 2. (5 баллов) \n",
    "При помощи gensim.models.Phrases реализуйте byte-pair-encoding, про который говорилось на первом семинаре (https://github.com/mannefedov/compling_nlp_hse_course/blob/master/notebooks/Preprocessing.ipynb) \n",
    "А именно 1) возьмите любой текст; разбейте его на предложения, а каждое предложение разбейте на отдельные символы (не потеряйте пробелы) 2) обучите gensim.models.Phrases на полученных символьных предложениях 3) примените полученный нграммер к этим символьным предложениям 4) повторите 2 и 3 N количество раз, чтобы начали получаться целые слова\n",
    "Параметры в gensim.models.Phrases влияют на количество получаемых нграммов после каждого прохода, поэтому не забудьте их настроить\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol_sents = [' '.join(sent) for sent in sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol_sents = [[ch for ch in sent if ch not in ',.;!?\\n'] for sent in symbol_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def symbol_grams(sents, iterations):\n",
    "    transformed = []\n",
    "    for _ in range(iterations):\n",
    "        if not transformed:\n",
    "            transformed = sents\n",
    "        phrases = Phrases(transformed, scoring='npmi', threshold=0, min_count=2)\n",
    "        transformed = phrases[transformed]\n",
    "    return transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = symbol_grams(symbol_sents, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видим, действительно, используя такой метод, не получается получить нграммы соответствующие именно отдельным словам, не перепрыгивая через пробелы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['V_a_l_e_n_t_i_n',\n",
       " 'e_ _M_i_c_h_a',\n",
       " 'e_l_ _S_m_i_t_h',\n",
       " ' _w_a_s_ _a_s_ ',\n",
       " 'r_e_a_l_ _a_s_ ',\n",
       " 't_a_x_e_s_ _b',\n",
       " 'u_t_ _h_e_ _w_a',\n",
       " 's_ _a_ _r',\n",
       " 'a_c_e_ _o_f_ ',\n",
       " 'o_n_e']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(result)[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_spaces(sents):\n",
    "    res = []\n",
    "    for sent in sents:\n",
    "        sub = []\n",
    "        for word in sent:\n",
    "            sub.extend(word.split(' '))\n",
    "        sub = [word.strip('_')  if word else ' ' for word in sub] \n",
    "        res.append(sub)   \n",
    "    return res    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def symbol_grams_spaces(sents, iterations):\n",
    "    transformed = []\n",
    "    for _ in range(iterations):\n",
    "        if not transformed:\n",
    "            transformed = sents\n",
    "        phrases = Phrases(transformed, scoring='npmi', threshold=0, min_count=3)\n",
    "        transformed = phrases[transformed]\n",
    "        transformed = split_spaces(transformed)\n",
    "    return transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_spaces = symbol_grams_spaces(symbol_sents, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С учётом пробелов получается результат, более близкий к желаемому, но появляются дополнительные пробелы. Предположительно, это связано с тем, что артикль склеивается с пробелом при каждой итерации, что вполне объяснимо тем, что для артикля это самая частотная пара."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'V_a_l_e_n_t_i_n|e| |M_i_c_h_a_e_l| |S|m_i_t_h| |w_a_s| |a_s| |r_e|a_l| |a_s| |t_a_x_e_s| |b|u_t| |h_e| |w_a_s| |a| | | | | | | |r_a_c|e| |o_f| | |o_n|e'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'|'.join(list(result_spaces)[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
