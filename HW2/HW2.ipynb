{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание 1. (5 баллов) \n",
    "В тетрадке реализована биграмная языковая модель (при генерации учитывается информация только о 1 предыдущем слове). Реализуйте триграмную модель и сгенерируйте несколько текстов. Сравните их с текстами, сгенерированными биграмной моделью. \n",
    "Можно использовать те же тексты, что в семинаре, или взять какой-то другой (на английском или русском языке).  \n",
    "\n",
    "Делать это задание будет легче после прочтения первых 7 страниц вот этой главы из Журафского - https://web.stanford.edu/~jurafsky/slp3/3.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "from gensim.models.phrases import Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stranger.txt', encoding='utf-8') as file: #Robert Heinlein's Stranger in a Strange Land with preface removed\n",
    "    stranger = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "stranger = stranger.replace('“Stranger In A Strange Land” by Robert Heinlein', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = nltk.tokenize.sent_tokenize(stranger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Не включаю слова, написанные через дефис, т.к. 1) в данном тексте не используются тире 2) между дефисами нет пробелов.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r'([A-Za-z]+[\\']?[A-Za-z]*)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = [re.findall(pattern, sent) for sent in sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "tri_model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "for sentence in sents:\n",
    "    for w1, w2, w3 in nltk.trigrams(sentence, pad_right=True, pad_left=True, left_pad_symbol='<s>', right_pad_symbol='</s>'):\n",
    "        tri_model[(w1, w2)][w3] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Логарифм не использую, потому что верятности нужны только, чтобы сделать взвешенную выборку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bigram in tri_model:\n",
    "    total_count = sum(tri_model[bigram].values())\n",
    "    for target in tri_model[bigram]:\n",
    "        tri_model[bigram][target] /= total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tri_generate(model, start=('<s>', '<s>')):\n",
    "    text = list(start)\n",
    "    while text[-1] != '</s>': \n",
    "        index = tuple(text[-2:])\n",
    "        keys = list(model[index].keys())\n",
    "        values = list(model[index].values())\n",
    "        key = np.random.choice(keys, 1, values)[0]\n",
    "        text.append(key)\n",
    "    return ' '.join(text[2:]).strip(' </s>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_generator(sent_generator, model, number_of_sents=1, count_words=False):\n",
    "    result = []\n",
    "    for _ in range(number_of_sents):\n",
    "        result.append(sent_generator(model))\n",
    "    if count_words == True:\n",
    "        count = count_words_avg(result)\n",
    "        return count\n",
    "    else:\n",
    "        result = '. '.join(result) + '.'\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words_avg(sents):\n",
    "    total = 0\n",
    "    for sent in sents:\n",
    "        total += len(sent.split(' '))\n",
    "    return total/len(sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Тут надо отметить, что данный текст – результат работы программы автоматического распознавания печатной книги, поэтому здесь нередко встречаются ошибки. Их можно было бы поправить, но это в масштаб данного задания всё же не входит.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Supreme Bishop and you won't believe it was still climbing. Lord love you it's a short delay from the Secretary won't come in through here. Finally Jubal said I'll stick to stairs and to let the infant with both of genius level. Moreover he could figure a deal that might balk them. Fly. Stinky taught it in Mike many times each day.\""
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tri_example = text_generator(tri_generate, tri_model, 6)\n",
    "tri_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "for sentence in sents:\n",
    "    for w1, w2 in nltk.bigrams(sentence, pad_right=True, pad_left=True, left_pad_symbol='<s>', right_pad_symbol='</s>'):\n",
    "        bi_model[w1][w2] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "for unigram in bi_model:\n",
    "    total_count = sum(bi_model[unigram].values())\n",
    "    for target in bi_model[unigram]:\n",
    "        bi_model[unigram][target] /= total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bi_generate(model, start=['<s>']):\n",
    "    text = copy.copy(start)\n",
    "    while text[-1] != '</s>': \n",
    "        index = text[-1]\n",
    "        keys = list(model[index].keys())\n",
    "        values = list(model[index].values())\n",
    "        key = np.random.choice(keys, 1, values)[0]\n",
    "        text.append(key)\n",
    "    return ' '.join(text[1:]).strip(' </s>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Nevertheless they supported their priest someday and anned guard placed herself he manages God didn't speak wrongly at Jubal's favorite children Kids in dressed up of Douglas concluded still places half century of truth interwoven with weak spot in heat lightning bug. VI had his aid kits in how darling I'm slow motion monestary garden area around them their weapons disappear one longish pause. Kiss quite incapable of grace of fourthrate prophet who hold even removed his freezer if Captain and went there. Beginners' classes for town suddenly reminded himself tightly when to alcoholic brotherhood. Martinis Think it there can save you catch his major skills although I ride in JubaL he protested shrilly Where's my forties whom Harshaw reached the dictionary which often needs your religion claims her against earlier hour ago just say wearing was already lived as hard part II before even talk Jubal knowing nothing to evade it bliss because it's that s three each differently. Can't say something later after is told that word swim in twosome partnerships inside line of natural Aunt Patty doesn't sleep court You asked of Health had brought it senile.\""
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_example = text_generator(bi_generate, bi_model, 6)\n",
    "bi_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат триграммной модели для сравнения: <br> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Supreme Bishop and you won't believe it was still climbing. Lord love you it's a short delay from the Secretary won't come in through here. Finally Jubal said I'll stick to stairs and to let the infant with both of genius level. Moreover he could figure a deal that might balk them. Fly. Stinky taught it in Mike many times each day.\""
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tri_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно отметить две вещи: <br>\n",
    "1) Предложения, созданные триграммной модели определённо больше похожи на текст, написанный человеком. Они более связные (только грамматически, естественно). <br>\n",
    "2) Биграммные предложения длиннее, чем триграммные, при условии, что мы останавливаемся только на символе окончания предложения. Это подтверждается экспериментом ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average bigram model sentence length: 19.59 words\n",
      "Average trigram model sentence length: 12.00 words\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n = 100\n",
    "m = 10\n",
    "bi_test = 0\n",
    "tri_test = 0\n",
    "for _ in range(n):\n",
    "    bi_test += text_generator(bi_generate, bi_model, m, count_words=True)\n",
    "    tri_test += text_generator(tri_generate, tri_model, m, count_words=True)\n",
    "print(f'Average bigram model sentence length: {bi_test/n:.2f} words\\n' +\n",
    "      f'Average trigram model sentence length: {tri_test/n:.2f} words\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание 2. (5 баллов) \n",
    "При помощи gensim.models.Phrases реализуйте byte-pair-encoding, про который говорилось на первом семинаре (https://github.com/mannefedov/compling_nlp_hse_course/blob/master/notebooks/Preprocessing.ipynb) \n",
    "А именно 1) возьмите любой текст; разбейте его на предложения, а каждое предложение разбейте на отдельные символы (не потеряйте пробелы) 2) обучите gensim.models.Phrases на полученных символьных предложениях 3) примените полученный нграммер к этим символьным предложениям 4) повторите 2 и 3 N количество раз, чтобы начали получаться целые слова\n",
    "Параметры в gensim.models.Phrases влияют на количество получаемых нграммов после каждого прохода, поэтому не забудьте их настроить\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol_sents = [' '.join(sent) for sent in sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol_sents = [[ch for ch in sent if ch not in ',.;!?\\n'] for sent in symbol_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def symbol_grams(sents, iterations):\n",
    "    transformed = []\n",
    "    for _ in range(iterations):\n",
    "        if not transformed:\n",
    "            transformed = sents\n",
    "        phrases = Phrases(transformed, scoring='npmi', threshold=0, min_count=2)\n",
    "        transformed = phrases[transformed]\n",
    "    return transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = symbol_grams(symbol_sents, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видим, действительно, используя такой метод, не получается получить нграммы соответствующие именно отдельным словам, не перепрыгивая через пробелы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['V_a_l_e_n_t_i_n',\n",
       " 'e_ _M_i_c_h_a',\n",
       " 'e_l_ _S_m_i_t_h',\n",
       " ' _w_a_s_ _a_s_ ',\n",
       " 'r_e_a_l_ _a_s_ ',\n",
       " 't_a_x_e_s_ _b',\n",
       " 'u_t_ _h_e_ _w_a',\n",
       " 's_ _a_ _r',\n",
       " 'a_c_e_ _o_f_ ',\n",
       " 'o_n_e']"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(result)[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_spaces(sents):\n",
    "    res = []\n",
    "    for sent in sents:\n",
    "        sub = []\n",
    "        for word in sent:\n",
    "            sub.extend(word.split(' '))\n",
    "        sub = [word.strip('_')  if word else ' ' for word in sub] \n",
    "        res.append(sub)   \n",
    "    return res    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def symbol_grams_spaces(sents, iterations):\n",
    "    transformed = []\n",
    "    for _ in range(iterations):\n",
    "        if not transformed:\n",
    "            transformed = sents\n",
    "        phrases = Phrases(transformed, scoring='npmi', threshold=0, min_count=3)\n",
    "        transformed = phrases[transformed]\n",
    "        transformed = split_spaces(transformed)\n",
    "    return transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_spaces = symbol_grams_spaces(symbol_sents, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С учётом пробелов получается результат, более близкий к желаемому, но появляются дополнительные пробелы. Предположительно, это связано с тем, что артикль склеивается с пробелом при каждой итерации, что вполне объяснимо тем, что для артикля это самая частотная пара."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'V_a_l_e_n_t_i_n|e| |M_i_c_h_a_e_l| |S|m_i_t_h| |w_a_s| |a_s| |r_e|a_l| |a_s| |t_a_x_e_s| |b|u_t| |h_e| |w_a_s| |a| | | | | | | |r_a_c|e| |o_f| | |o_n|e'"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'|'.join(list(result_spaces)[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
