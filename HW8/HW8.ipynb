{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Домашнее задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В семинаре использовался только небольшой кусочек данных. На всех данных пересчитайте baseline (tfidf). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ваша задача - предложить 3 способа побить бейзлайн на всех данных.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нет никаких ограничений кроме:\n",
    "\n",
    "1) нельзя изменять метрику  \n",
    "2) решение должно быть воспроизводимым  \n",
    "3) способы дожны отличаться друг от друга не только гиперпараметрами (например, нельзя три раза поменять гиперпарамтры в TfidfVectorizer и сдать работу)  \n",
    "4) изменение количества извлекаемых слов не является улучшением (выберите одно значение и используйте только его)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве ответа нужно предоставить jupyter тетрадку с экспериментами (обязательное условие!) и описать каждую из идей в форме - https://forms.gle/GWzewBEpw8qnkv8t8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждый реализованный и описанный способ оценивается в 3 балла. Дополнительный балл можно получить, если способы затрагивают разные аспекты решения (например, первая идея - улучшить нормализацию, вторая - улучшить способ представления текста в виде графа, третья - предложить способ удаления из топа идентичных ключевых слов (рф, россия))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно использовать мой код как основу, а можно придумать что-то полностью другое."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если у вас никак не получается побить бейзлайн вы можете предоставить реализацию и описание неудавшихся экспериментов (каждый оценивается в 1 балл)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В поисках идей можно почитать обзоры по теме (посмотрите еще статьи, в которых цитируются эти обзоры): https://www.semanticscholar.org/search?year%5B0%5D=2012&year%5B1%5D=2020&publicationType%5B0%5D=Reviews&q=keyword%20extraction&sort=relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Использовать доступные готовые решения тоже можно**. Так что погуглите перед тем, как приступать к заданию. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для обработки решил использовать майстем, т.к. он работате немного быстрее."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку процесс обработчки всего датасета оказался очень длительным, я сохранил результаты в отдельный файл и далее использовал его. Обработку закомментил."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Mystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_english(s):\n",
    "    try:\n",
    "        s.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Берем только существительные. Если слово написано латиницей, то возвращаем без изменений, т.е. предполагаем, что это термин."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noun(result):\n",
    "    try:\n",
    "        if 'analysis' in result.keys():\n",
    "            if not result['analysis']:\n",
    "                text = result['text']\n",
    "                if is_english(text):\n",
    "                    return text.lower()\n",
    "                else:\n",
    "                    return None\n",
    "            elif result['analysis'][0]['gr'][:2] == 'S,':\n",
    "                return result['analysis'][0]['lex']\n",
    "            else:\n",
    "                return None\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    \n",
    "    words = m.analyze(text)\n",
    "    words = [get_noun(word) for word in words]\n",
    "    words = [word for word in words if word]\n",
    "    words = [word for word in words if len(word) > 1]\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = './data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files = [os.path.join(PATH_TO_DATA, file) for file in os.listdir(PATH_TO_DATA)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.concat([pd.read_json(file, lines=True) for file in files],\n",
    "#                  axis=0,\n",
    "#                  ignore_index=True, \n",
    "#                  sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.drop(['abstract', 'url'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lower(words):\n",
    "    return [word.lower() for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['keywords'] = data['keywords'].apply(to_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# data['content_norm'] = data['content'].apply(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# data['title_norm'] = data['title'].apply(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.to_csv('normalized.csv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем нормализованные данные из файла для экономии времени."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('normalized.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пандас не умеет читать списки из напрямую из csv, поэтому преобразуем нужные столбцы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['title_norm'] = data['title_norm'].apply(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['content_norm'] = data['content_norm'].apply(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['keywords'] = data['keywords'].apply(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>keywords</th>\n",
       "      <th>summary</th>\n",
       "      <th>title</th>\n",
       "      <th>content_norm</th>\n",
       "      <th>title_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Действия России, якобы совершившей кибератаки ...</td>\n",
       "      <td>[выборы в сша, сша, санкции, хакеры, эксклюзив...</td>\n",
       "      <td>В США потребовали осудить Россию за то, что по...</td>\n",
       "      <td>Снова «русские хакеры»: сенат Иллинойса потреб...</td>\n",
       "      <td>[действие, россия, кибератака, комиссия, штат,...</td>\n",
       "      <td>[хакер, сенат, иллинойс, россия, кибератака]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Президент Латвии Раймонд Вейонис предлагает пр...</td>\n",
       "      <td>[гражданство, ес, латвия, национализм, права ч...</td>\n",
       "      <td>Президент Латвии Раймонд Вейонис предлагает пр...</td>\n",
       "      <td>Граждане под вопросом: зачем президент Латвии ...</td>\n",
       "      <td>[президент, латвия, раймонд, вейонис, ребенок,...</td>\n",
       "      <td>[гражданин, вопрос, президент, латвия, паспорт...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Совет Безопасности ООН единогласно принял резо...</td>\n",
       "      <td>[антониу гутерреш , василий небензя, вооруженн...</td>\n",
       "      <td>Совбез ООН единогласно принял резолюцию 2401 о...</td>\n",
       "      <td>Гуманитарная пауза: как могут развиваться собы...</td>\n",
       "      <td>[совет, безопасность, оон, резолюция, сторона,...</td>\n",
       "      <td>[пауза, событие, сирия, принятие, резолюция, с...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Выиграть главный футбольный матч клубного сезо...</td>\n",
       "      <td>[в мире, испания, италия, лига чемпионов уефа,...</td>\n",
       "      <td>Голкипер «Ювентуса» Джанлуиджи Буффон дважды п...</td>\n",
       "      <td>Попытка номер три, или Последний шанс: Буффон ...</td>\n",
       "      <td>[матч, сезон, европа, одиночка, вратарь, мир, ...</td>\n",
       "      <td>[попытка, номер, шанс, буффон, карьера, лига, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Украинским военным стоит быть весьма бдительны...</td>\n",
       "      <td>[военная техника, киев, корабль, крым, россия,...</td>\n",
       "      <td>В Киеве выразили опасения в связи с возвращени...</td>\n",
       "      <td>«Это уже паранойя»: в России прокомментировали...</td>\n",
       "      <td>[военный, техника, крым, россия, корабль, пере...</td>\n",
       "      <td>[паранойя, россия, заявление, украина, передач...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0  Действия России, якобы совершившей кибератаки ...   \n",
       "1  Президент Латвии Раймонд Вейонис предлагает пр...   \n",
       "2  Совет Безопасности ООН единогласно принял резо...   \n",
       "3  Выиграть главный футбольный матч клубного сезо...   \n",
       "4  Украинским военным стоит быть весьма бдительны...   \n",
       "\n",
       "                                            keywords  \\\n",
       "0  [выборы в сша, сша, санкции, хакеры, эксклюзив...   \n",
       "1  [гражданство, ес, латвия, национализм, права ч...   \n",
       "2  [антониу гутерреш , василий небензя, вооруженн...   \n",
       "3  [в мире, испания, италия, лига чемпионов уефа,...   \n",
       "4  [военная техника, киев, корабль, крым, россия,...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  В США потребовали осудить Россию за то, что по...   \n",
       "1  Президент Латвии Раймонд Вейонис предлагает пр...   \n",
       "2  Совбез ООН единогласно принял резолюцию 2401 о...   \n",
       "3  Голкипер «Ювентуса» Джанлуиджи Буффон дважды п...   \n",
       "4  В Киеве выразили опасения в связи с возвращени...   \n",
       "\n",
       "                                               title  \\\n",
       "0  Снова «русские хакеры»: сенат Иллинойса потреб...   \n",
       "1  Граждане под вопросом: зачем президент Латвии ...   \n",
       "2  Гуманитарная пауза: как могут развиваться собы...   \n",
       "3  Попытка номер три, или Последний шанс: Буффон ...   \n",
       "4  «Это уже паранойя»: в России прокомментировали...   \n",
       "\n",
       "                                        content_norm  \\\n",
       "0  [действие, россия, кибератака, комиссия, штат,...   \n",
       "1  [президент, латвия, раймонд, вейонис, ребенок,...   \n",
       "2  [совет, безопасность, оон, резолюция, сторона,...   \n",
       "3  [матч, сезон, европа, одиночка, вратарь, мир, ...   \n",
       "4  [военный, техника, крым, россия, корабль, пере...   \n",
       "\n",
       "                                          title_norm  \n",
       "0       [хакер, сенат, иллинойс, россия, кибератака]  \n",
       "1  [гражданин, вопрос, президент, латвия, паспорт...  \n",
       "2  [пауза, событие, сирия, принятие, резолюция, с...  \n",
       "3  [попытка, номер, шанс, буффон, карьера, лига, ...  \n",
       "4  [паранойя, россия, заявление, украина, передач...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(true_kws, predicted_kws):\n",
    "    assert len(true_kws) == len(predicted_kws)\n",
    "    \n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    jaccards = []\n",
    "    \n",
    "    for i in range(len(true_kws)):\n",
    "        true_kw = set(true_kws[i])\n",
    "        predicted_kw = set(predicted_kws[i])\n",
    "        \n",
    "        tp = len(true_kw & predicted_kw)\n",
    "        union = len(true_kw | predicted_kw)\n",
    "        fp = len(predicted_kw - true_kw)\n",
    "        fn = len(true_kw - predicted_kw)\n",
    "        \n",
    "        if (tp+fp) == 0:\n",
    "            prec = 0\n",
    "        else:\n",
    "            prec = tp / (tp + fp)\n",
    "        \n",
    "        if (tp+fn) == 0:\n",
    "            rec = 0\n",
    "        else:\n",
    "            rec = tp / (tp + fn)\n",
    "        if (prec+rec) == 0:\n",
    "            f1 = 0\n",
    "        else:\n",
    "            f1 = (2*(prec*rec))/(prec+rec)\n",
    "            \n",
    "        jac = tp / union\n",
    "        \n",
    "        precisions.append(prec)\n",
    "        recalls.append(rec)\n",
    "        f1s.append(f1)\n",
    "        jaccards.append(jac)\n",
    "    print('Precision - ', round(np.mean(precisions), 2))\n",
    "    print('Recall - ', round(np.mean(recalls), 2))\n",
    "    print('F1 - ', round(np.mean(f1s), 2))\n",
    "    print('Jaccard - ', round(np.mean(jaccards), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['content_norm_str'] = data['content_norm'].apply(' '.join)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если после нормализации ничего не осталось, то сравнивать не имеет смысла."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['content_norm_str'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь пришлось задать максимальное количество фич, потому что иначе не хватает памяти."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1, 2), min_df=5, max_df=0.9, max_features=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.9, max_features=600, min_df=5,\n",
       "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.fit(data['content_norm_str'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = {i:word for i,word in enumerate(tfidf.get_feature_names())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_vectors = tfidf.transform(data['content_norm_str'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем подобрать оптимальное количество слов, получаемых данным методом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [[id2word[w] for w in top] for top in texts_vectors.toarray().argsort()[:,:-11:-1]] #10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.07\n",
      "Recall -  0.08\n",
      "F1 -  0.07\n",
      "Jaccard -  0.04\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [[id2word[w] for w in top] for top in texts_vectors.toarray().argsort()[:,:-6:-1]] #5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.1\n",
      "Recall -  0.06\n",
      "F1 -  0.07\n",
      "Jaccard -  0.04\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [[id2word[w] for w in top] for top in texts_vectors.toarray().argsort()[:,:-8:-1]] #7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.08\n",
      "Recall -  0.07\n",
      "F1 -  0.07\n",
      "Jaccard -  0.04\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [[id2word[w] for w in top] for top in texts_vectors.toarray().argsort()[:,:-16:-1]] #15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.05\n",
      "Recall -  0.1\n",
      "F1 -  0.07\n",
      "Jaccard -  0.04\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если берем слишком маленькое или слишком большое, то показатели падают. Для 7 и 10 зеркальные значения точности и полноты. В последующих алгоритмах будем использовать **10** слов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, показатели данного метода на всем датасете оказались ниже, чем были на маленьком кусочке. Итоговый бейзлайн:<br><br>\n",
    "Precision -  0.07<br>\n",
    "Recall -  0.08<br>\n",
    "F1 -  0.07<br>\n",
    "Jaccard -  0.04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Способ 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем улучшить результат, меняя параметры tfidf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1, 2), min_df=3, max_df=0.8, max_features=1000, sublinear_tf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.8, max_features=1000, min_df=3,\n",
       "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=True,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.fit(data['content_norm_str'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = {i:word for i,word in enumerate(tfidf.get_feature_names())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_vectors = tfidf.transform(data['content_norm_str'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [[id2word[w] for w in top] for top in texts_vectors.toarray().argsort()[:,:-11:-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удалось получить очень небольшое улучшение в качестве."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.08\n",
      "Recall -  0.1\n",
      "F1 -  0.08\n",
      "Jaccard -  0.05\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tfidf\n",
    "del keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Способ 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем pagerank на всем датасете."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавил возможность сделать граф направленным."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_matrix(text, window_size=5, directed=False):\n",
    "    vocab = set(text)\n",
    "    word2id = {w:i for i, w in enumerate(vocab)}\n",
    "    id2word = {i:w for i, w in enumerate(vocab)}\n",
    "    ids = [word2id[word] for word in text]\n",
    "\n",
    "    m = np.zeros((len(vocab), len(vocab)))\n",
    "\n",
    "    for i in range(0, len(ids), window_size):\n",
    "        window = ids[i:i+window_size]\n",
    "        for j, k in combinations(window, 2):\n",
    "            m[j][k] += 1\n",
    "            if not directed:\n",
    "                m[k][j] += 1\n",
    "    \n",
    "    return m, id2word\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagerank(text, window_size=5, topn=5, directed=False):\n",
    "    \n",
    "    matrix, id2word = build_matrix(text, window_size, directed)\n",
    "    G = nx.from_numpy_array(matrix)\n",
    "    node2measure = dict(nx.pagerank(G))\n",
    "    \n",
    "    return [id2word[index] for index, measure in sorted(node2measure.items(), key=lambda x: -x[1])[:topn]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вообще pagerank предполагает все жё направленный граф, поэтому есть смысл сравнивать оба варианта. Результаты на всякий случай сохраняю в отдельной текстовой ячейке, потому что на таком объеме данных алгоритм работает очень долго и не очень удобно каждый раз заново прогонять функцию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для сравнения попробовал использовать по 5 и 10 слов в направленных и ненаправленных графах. Используя 5 слов на маленьком кусочке, у направленного графа маленько преимущество."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.25\n",
      "Recall -  0.14\n",
      "F1 -  0.17\n",
      "Jaccard -  0.1\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'][:100], data['content_norm'][:100].apply(lambda x: pagerank(x, topn=5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.26\n",
      "Recall -  0.14\n",
      "F1 -  0.18\n",
      "Jaccard -  0.1\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'][:100], data['content_norm'][:100].apply(lambda x: pagerank(x, topn=5, directed=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.19\n",
      "Recall -  0.21\n",
      "F1 -  0.19\n",
      "Jaccard -  0.11\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'][:100], data['content_norm'][:100].apply(lambda x: pagerank(x, topn=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.19\n",
      "Recall -  0.21\n",
      "F1 -  0.19\n",
      "Jaccard -  0.11\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'][:100], data['content_norm'][:100].apply(lambda x: pagerank(x, topn=10, directed=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для чистоты эксперимента пройдемся по всем данным, выбирая по 10 слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_pg = data['content_norm'].apply(lambda x: pagerank(x, topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.12\n",
      "Recall -  0.17\n",
      "F1 -  0.13\n",
      "Jaccard -  0.08\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], keywords_pg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ненаправленный граф, 10 слов.<br><br>\n",
    "Precision -  0.12 <br>\n",
    "Recall -  0.17 <br>\n",
    "F1 -  0.13 <br>\n",
    "Jaccard -  0.08 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_pg_dir = data['content_norm'].apply(lambda x: pagerank(x, topn=10, directed=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.12\n",
      "Recall -  0.17\n",
      "F1 -  0.13\n",
      "Jaccard -  0.08\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], keywords_pg_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Направленный граф, 10 слов. <br><br>\n",
    "Precision -  0.12 <br>\n",
    "Recall -  0.17 <br>\n",
    "F1 -  0.13 <br>\n",
    "Jaccard -  0.08 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Похоже, что в данном решении направленность роли не играет."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Способ 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем простую меру центральности для сравнения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centrality_degree_measure(text, window_size=5, topn=5, directed=False):\n",
    "    \n",
    "    matrix, id2word = build_matrix(text, window_size, directed)\n",
    "    G = nx.from_numpy_array(matrix)\n",
    "    node2measure = dict(nx.degree_centrality(G))\n",
    "    \n",
    "    return [id2word[index] for index,measure in sorted(node2measure.items(), key=lambda x: -x[1])[:topn]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_deg = data['content_norm'].apply(lambda x: centrality_degree_measure(x, topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.12\n",
      "Recall -  0.17\n",
      "F1 -  0.13\n",
      "Jaccard -  0.08\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], keywords_deg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данный алгоритм работает быстро, поэтому можно попробовать разные параметры."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Направленность"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_deg = data['content_norm'].apply(lambda x: centrality_degree_measure(x, topn=10, directed=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.12\n",
      "Recall -  0.17\n",
      "F1 -  0.13\n",
      "Jaccard -  0.08\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], keywords_deg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Размер окна"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_deg = data['content_norm'].apply(lambda x: centrality_degree_measure(x, 10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.12\n",
      "Recall -  0.17\n",
      "F1 -  0.13\n",
      "Jaccard -  0.08\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], keywords_deg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что в нашей задаче эти параметры ничего не меняют. Получаем тот же результат, но заметно быстрее, чем pagerank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Способ 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм и готовая реализация <a href=\"https://github.com/LIAAD/yake\">Yet Another Keyword Extractor (Yake)</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сразу видно, что выдаваемые результаты действительно похожи на имеющиеся ключевые слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('гражданство', 0.0036933995951219712)\n",
      "('латвия', 0.0038674353118549982)\n",
      "('негражданин', 0.003870623870057238)\n",
      "('ребенок', 0.004910473600299539)\n",
      "('год', 0.006635017495495632)\n",
      "('человек', 0.007815788396135817)\n",
      "('государство', 0.008844585219468052)\n",
      "('язык', 0.009710213770363164)\n",
      "('страна', 0.009710213770363164)\n",
      "('право', 0.010620942749700641)\n"
     ]
    }
   ],
   "source": [
    "language = \"ru\"\n",
    "max_ngram_size = 1\n",
    "deduplication_thresold = 0.9\n",
    "deduplication_algo = 'seqm'\n",
    "windowSize = 1\n",
    "numOfKeywords = 10\n",
    "\n",
    "custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_thresold, dedupFunc=deduplication_algo, windowsSize=windowSize, top=numOfKeywords, features=None)\n",
    "keywords = custom_kw_extractor.extract_keywords(data.content_norm_str[1])\n",
    "\n",
    "for kw in keywords:\n",
    "    print(kw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yake_run(text, topn=10, max_ngram=1):\n",
    "    language = \"ru\"\n",
    "    max_ngram_size = max_ngram\n",
    "    deduplication_thresold = 0.9\n",
    "    deduplication_algo = 'seqm'\n",
    "    windowSize = 2\n",
    "    numOfKeywords = topn\n",
    "\n",
    "    custom_kw_extractor = yake.KeywordExtractor(lan=language,\n",
    "                                                n=max_ngram_size,\n",
    "                                                dedupLim=deduplication_thresold,\n",
    "                                                dedupFunc=deduplication_algo,\n",
    "                                                windowsSize=windowSize, top=numOfKeywords, features=None)\n",
    "    keywords = custom_kw_extractor.extract_keywords(text)\n",
    "    # The lower the score, the more relevant the keyword is.\n",
    "    keywords = [kw[0] for kw in keywords]\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.19\n",
      "Recall -  0.2\n",
      "F1 -  0.19\n",
      "Jaccard -  0.11\n",
      "Wall time: 14.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "evaluate(data.keywords[:100], data.content_norm_str[:100].apply(yake_run))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.03\n",
      "Recall -  0.02\n",
      "F1 -  0.02\n",
      "Jaccard -  0.01\n",
      "Wall time: 19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "evaluate(data.keywords[:100], data.content_norm_str[:100].apply(yake_run, max_ngram=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очевидно, что нграмы с длиной больше одного в рамках имеющейся метрики не очень хорошо работают."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_yake = data.content_norm_str.apply(yake_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.12\n",
      "Recall -  0.17\n",
      "F1 -  0.13\n",
      "Jaccard -  0.08\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], keywords_yake)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И опять получаем идентичный результат."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision -  0.12 <br>\n",
    "Recall -  0.17 <br>\n",
    "F1 -  0.13 <br>\n",
    "Jaccard -  0.08 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
