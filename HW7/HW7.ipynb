{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.core.display import display, HTML\n",
    "# display(HTML(\"<style>.container { width:85% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import nltk\n",
    "import itertools\n",
    "\n",
    "\n",
    "import adagram\n",
    "from lxml import html\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from string import punctuation\n",
    "import json, os\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "morph = MorphAnalyzer()\n",
    "punct = punctuation+'«»—…“”*№–'\n",
    "stops = set(stopwords.words('russian'))\n",
    "\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    global stops\n",
    "    words = [word.strip(punct) for word in text.lower().split() if word and word not in stops]\n",
    "    words = [word for word in words if word]\n",
    "\n",
    "    return words\n",
    "\n",
    "def normalize(text):\n",
    "    \n",
    "    words = tokenize(text)\n",
    "    words = [morph.parse(word)[0].normal_form for word in words if word]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание № 1. Протестировать адаграм в определении перефразирования"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Векторизуйте пары текстов с помощью Адаграма, обучите любую модель и оцените качество (кросс-валидацией). \n",
    "\n",
    "За основу возьмите код из предыдущего семинара/домашки, только в функции get_embedding вам нужно выбирать вектор нужного значения (импользуйте model.disambiguate и model.sense_vector). Отдельные векторы усредните как и в предыдущем семинаре."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('paraphraser/paraphrases.xml', 'rb') as file:\n",
    "    corpus_xml = html.fromstring(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_1 = []\n",
    "texts_2 = []\n",
    "classes = []\n",
    "\n",
    "for p in corpus_xml.xpath('//paraphrase'):\n",
    "    texts_1.append(p.xpath('./value[@name=\"text_1\"]/text()')[0])\n",
    "    texts_2.append(p.xpath('./value[@name=\"text_2\"]/text()')[0])\n",
    "    classes.append(p.xpath('./value[@name=\"class\"]/text()')[0])\n",
    "    \n",
    "data = pd.DataFrame({'text_1':texts_1, 'text_2':texts_2, 'label':classes})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm = adagram.VectorModel.load(\"out.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.9999317918286611)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vm.word_sense_probs('земля')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.89643997e-04, 9.99510356e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vm.disambiguate('мир', ['новый', 'славный'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_1_norm'] = data['text_1'].apply(normalize)\n",
    "data['text_2_norm'] = data['text_2'].apply(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, [1, 2, 3]],\n",
       " [1, [0, 2, 3, 4]],\n",
       " [2, [0, 1, 3, 4, 5]],\n",
       " [3, [0, 1, 2, 4, 5, 6]],\n",
       " [4, [1, 2, 3, 5, 6, 7]],\n",
       " [5, [2, 3, 4, 6, 7, 8]],\n",
       " [6, [3, 4, 5, 7, 8, 9]],\n",
       " [7, [4, 5, 6, 8, 9]],\n",
       " [8, [5, 6, 7, 9]],\n",
       " [9, [6, 7, 8]]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_compare = [[0, [1, 2, 3]],\n",
    " [1, [0, 2, 3, 4]],\n",
    " [2, [0, 1, 3, 4, 5]],\n",
    " [3, [0, 1, 2, 4, 5, 6]],\n",
    " [4, [1, 2, 3, 5, 6, 7]],\n",
    " [5, [2, 3, 4, 6, 7, 8]],\n",
    " [6, [3, 4, 5, 7, 8, 9]],\n",
    " [7, [4, 5, 6, 8, 9]],\n",
    " [8, [5, 6, 7, 9]],\n",
    " [9, [6, 7, 8]]]\n",
    "\n",
    "to_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_in_context(words, window=3):\n",
    "    words_len = len(words)\n",
    "    words_in_context = []\n",
    "    for i in range(words_len):\n",
    "        word = words[i]\n",
    "        left_context = words[max(0, i - window):i]\n",
    "        right_context = words[i + 1:min(words_len, i + window + 1)]\n",
    "        context = left_context + right_context\n",
    "        words_in_context.append([word, context])\n",
    "    return words_in_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, [1, 2, 3]],\n",
       " [1, [0, 2, 3, 4]],\n",
       " [2, [0, 1, 3, 4, 5]],\n",
       " [3, [0, 1, 2, 4, 5, 6]],\n",
       " [4, [1, 2, 3, 5, 6, 7]],\n",
       " [5, [2, 3, 4, 6, 7, 8]],\n",
       " [6, [3, 4, 5, 7, 8, 9]],\n",
       " [7, [4, 5, 6, 8, 9]],\n",
       " [8, [5, 6, 7, 9]],\n",
       " [9, [6, 7, 8]]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_words_in_context(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_words_in_context(words) == to_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sense_vector(word, context, model):\n",
    "    most_likely_sense = model.disambiguate(word, context).argmax()\n",
    "    sense_vec = model.sense_vector(word, most_likely_sense)\n",
    "    return sense_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_adagram(text, model, window=3, dim=100):\n",
    "    \n",
    "    word2context = get_words_in_context(text, window)\n",
    "    \n",
    "    \n",
    "    vectors = np.zeros((len(word2context), dim))\n",
    "    \n",
    "    for i, (word, context) in enumerate(word2context):\n",
    "        \n",
    "        try:\n",
    "            v = get_sense_vector(word, context, model)\n",
    "            vectors[i] = v\n",
    "        \n",
    "        except (KeyError, ValueError):\n",
    "            continue\n",
    "    \n",
    "    if vectors.any():\n",
    "        vector = np.average(vectors, axis=0)\n",
    "    else:\n",
    "        vector = np.zeros((dim))\n",
    "    \n",
    "    return vector\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01474951,  0.15916309, -0.00517532,  0.07148482, -0.11557754,\n",
       "        0.00151955, -0.06875914, -0.04772396, -0.03014633,  0.11318449,\n",
       "       -0.05010392,  0.07272814,  0.06377294,  0.06653478,  0.10099462,\n",
       "       -0.00716796,  0.00876474,  0.18644314,  0.19070874, -0.00914995,\n",
       "       -0.07892463, -0.00369429, -0.07506094, -0.09971022, -0.1280301 ,\n",
       "       -0.08694893, -0.08521983, -0.06966679, -0.0385777 ,  0.00251285,\n",
       "       -0.06280887, -0.09142431,  0.02111946, -0.07578302, -0.04335929,\n",
       "        0.02006594, -0.03932559, -0.08505101, -0.13847218, -0.00089411,\n",
       "        0.06361622,  0.07585328,  0.0174655 ,  0.25460323, -0.0474697 ,\n",
       "        0.04015753,  0.01645942,  0.14489075,  0.04684166, -0.11665628,\n",
       "        0.00724955,  0.00441676, -0.05104768,  0.1117997 ,  0.03522702,\n",
       "       -0.04775833,  0.10879791, -0.0637923 ,  0.0181433 , -0.10177186,\n",
       "        0.20217654,  0.04528808, -0.00873872, -0.06189587, -0.00435417,\n",
       "       -0.00492449,  0.05316143,  0.02388541, -0.07703598, -0.11150189,\n",
       "       -0.10164156,  0.07450934, -0.14292615, -0.10452347,  0.04110906,\n",
       "       -0.01470117, -0.01106682,  0.12654082, -0.14031925, -0.01225855,\n",
       "       -0.1088596 ,  0.11194771, -0.0593363 , -0.08560282,  0.09883535,\n",
       "        0.15381132, -0.15876987, -0.00680239,  0.02165399,  0.02332486,\n",
       "        0.01845833, -0.14480204, -0.09034278, -0.0595537 , -0.01850392,\n",
       "       -0.18377455, -0.02110473, -0.02030728, -0.07178801, -0.08466787])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_embedding_adagram('тренировочный мир тестов', vm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text_1 = [get_embedding_adagram(text, vm) for text in data['text_1_norm']]\n",
    "X_text_2 = [get_embedding_adagram(text, vm) for text in data['text_2_norm']]\n",
    "\n",
    "X_text = np.concatenate([X_text_1, X_text_2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(C=1, solver='liblinear', penalty='l1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4258777919219092"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cross_val_score(clf, X_text, y, scoring='f1_micro', cv=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117659"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(wn.all_synsets()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2. Реализовать алгоритм Леска и проверить его на реальном датасете"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ворднет можно использовать для дизамбигуации. Самый простой алгоритм дизамбигуации - алгоритм Леска. В нём нужное значение слова находится через пересечение слов контекста, в котором употреблено это слово, с определениями значений слова из ворднета. Значение с максимальным пересечением - нужное."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):    \n",
    "    words = tokenize(text)\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word]   \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_examples(syn):\n",
    "    if syn.examples():\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_definition(syn):\n",
    "    if syn.definition():\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lesk(word, sentence):\n",
    "    bestsense = 0\n",
    "    maxoverlap = 0\n",
    "    \n",
    "    synsets = wn.synsets(word)\n",
    "    if len(synsets) < 2:\n",
    "        return bestsense\n",
    "    \n",
    "    sentence = set(sentence)\n",
    "    definitions_dict = {}\n",
    "    \n",
    "    for i, syns in enumerate(synsets):\n",
    "        if has_definition(syns):\n",
    "            definitions_dict[i] = set(normalize(syns.definition()))\n",
    "        else:\n",
    "            definitions_dict[i] = set()\n",
    "        \n",
    "    for i in range(len(definitions_dict)):\n",
    "        overlap = len(sentence.intersection(definitions_dict[i]))\n",
    "        if overlap > maxoverlap:\n",
    "            maxoverlap = overlap\n",
    "            bestsense = i\n",
    "    \n",
    "    return bestsense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('corpus_wsd_50k.txt') as file:\n",
    "    corpus = file.read().split('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_wsd = []\n",
    "for sent in corpus:\n",
    "    corpus_wsd.append([s.split('\\t') for s in sent.split('\\n')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_sentence(sentence):\n",
    "    lemmas = [word[1] for word in sentence]\n",
    "    word_contexts = get_words_in_context(lemmas)\n",
    "    total = 0 # number of ambiguous words in the sentence\n",
    "    correct = 0 # number of correctly predicted ambiguous words\n",
    "    for i, word_context in enumerate(word_contexts):\n",
    "        word = word_context[0]\n",
    "        context = word_context[1]\n",
    "        sense = sentence[i][0]\n",
    "        sense_predicted = lesk(word, context)\n",
    "        if sense:\n",
    "            total += 1\n",
    "            correct += validate_sense(word, sense, sense_predicted)\n",
    "    return correct, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_sense(word, sense, sense_predicted):\n",
    "    syn = wn.synsets(word)\n",
    "    \n",
    "    if syn:\n",
    "        prediction = syn[sense_predicted]\n",
    "    else:\n",
    "        prediction = None\n",
    "        \n",
    "    true_sense = wn.lemma_from_key(sense).synset()\n",
    "        \n",
    "#     print(prediction)\n",
    "#     print(true_sense)\n",
    "    \n",
    "    if not prediction:\n",
    "        return False\n",
    "    return prediction == true_sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 7)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_sentence(corpus_wsd[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['', 'how', 'How'],\n",
       " ['long%3:00:02::', 'long', 'long'],\n",
       " ['', 'have', 'has'],\n",
       " ['', 'it', 'it'],\n",
       " ['be%2:42:03::', 'be', 'been'],\n",
       " ['', 'since', 'since'],\n",
       " ['', 'you', 'you'],\n",
       " ['review%2:31:00::', 'review', 'reviewed'],\n",
       " ['', 'the', 'the'],\n",
       " ['objective%1:09:00::', 'objective', 'objectives'],\n",
       " ['', 'of', 'of'],\n",
       " ['', 'you', 'your'],\n",
       " ['benefit%1:21:00::', 'benefit', 'benefit'],\n",
       " ['', 'and', 'and'],\n",
       " ['service%1:04:07::', 'service', 'service'],\n",
       " ['program%1:09:01::', 'program', 'program'],\n",
       " ['', '?', '?']]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_wsd[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_lesk(n):\n",
    "    result = []\n",
    "    for sentence in corpus_wsd[:min(n, len(corpus_wsd))]:\n",
    "        result.append(eval_sentence(sentence))\n",
    "\n",
    "    result = np.array(result)\n",
    "    result_sum = np.sum(result, axis=0)\n",
    "    accuracy = result_sum[0]/result_sum[1]\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of current Lesk algorithm realisation is 0.5236189462480042\n",
      "CPU times: user 1min 15s, sys: 4.78 s, total: 1min 20s\n",
      "Wall time: 1min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(f'Accuracy of current Lesk algorithm realisation is {test_lesk(10000)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем доработать алгоритм, используя не только определения, но и примеры, если они есть."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lesk(word, sentence):\n",
    "    bestsense = 0\n",
    "    maxoverlap = 0\n",
    "    \n",
    "    synsets = wn.synsets(word)\n",
    "    if len(synsets) < 2:\n",
    "        return bestsense\n",
    "    \n",
    "    sentence = set(sentence)\n",
    "    context = {}\n",
    "    \n",
    "    for i, syns in enumerate(synsets):\n",
    "        if has_definition(syns):\n",
    "            context[i] = set(normalize(syns.definition()))\n",
    "            if has_examples(syns):\n",
    "                examples_set = set()\n",
    "                for example in syns.examples():\n",
    "                    examples_set.update(normalize(example))\n",
    "                context[i].update(examples_set)\n",
    "        else:\n",
    "            context[i] = set()\n",
    "        \n",
    "    for i in range(len(context)):\n",
    "        overlap = len(sentence.intersection(context[i]))\n",
    "        if overlap > maxoverlap:\n",
    "            maxoverlap = overlap\n",
    "            bestsense = i\n",
    "    \n",
    "    return bestsense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получаем небольшой прирост в качестве."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5309526343799894"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_lesk(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
