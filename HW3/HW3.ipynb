{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```1.``` Реализуйте алгоритм Symspell. Он похож на алгоритм Норвига, но проще и быстрее. Там к словам в словаре применяется только одна операция – удаление символа (1-n). Чтобы найти исправление из слова тоже удаляются символы и сравниваются с теми, что хранятся в словаре. Оцените качество полученного алгоритма теми же тремя метриками.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@wolfgarbe/1000x-faster-spelling-correction-algorithm-2012-8701fcd87a5f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from nltk import sent_tokenize\n",
    "import gzip\n",
    "import csv\n",
    "import pandas as pd\n",
    "import re\n",
    "from operator import itemgetter\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('lenta-ru-news.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_words(sent_1, sent_2):\n",
    "    tokens_1, tokens_2 = normalize(sent_1), normalize(sent_2)\n",
    "    return zip(tokens_1, tokens_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    pat = re.compile(r'[А-Яа-я]+')\n",
    "    normalized = re.findall(pat, text)\n",
    "\n",
    "    return [word.lower() for word in normalized if word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for text in texts[:12000]:\n",
    "    sents = sent_tokenize(text)\n",
    "    norm_sents = [normalize(sent) for sent in sents]\n",
    "    corpus += norm_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "\n",
    "for sent in corpus:\n",
    "    vocab.update(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = Counter()\n",
    "\n",
    "for sent in corpus:\n",
    "    words.update(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = dict(words)\n",
    "\n",
    "for key, value in word_freq.items():\n",
    "    word_freq[key] = value/len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_deletes_helper(word, min_len=0):\n",
    "    result = set()\n",
    "    # taking word length into account\n",
    "    if len(word) > min_len:\n",
    "        for i in range(len(word)):\n",
    "            result.add(word[:i] + word[i+1:])\n",
    "    else:\n",
    "        result.add(word)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# нужно сохранять предыдущие удаления!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_deletes(word, edit_distance=2, min_len=2):\n",
    "    edit_distance -= 1\n",
    "    forms = list(n_deletes_helper(word, min_len))\n",
    "    tmp = set(forms)\n",
    "    while edit_distance > 0:\n",
    "        edit_distance -= 1\n",
    "        new_forms = []\n",
    "        for form in tmp:\n",
    "            new_forms.extend(n_deletes_helper(form, min_len))\n",
    "        tmp = set(new_forms)\n",
    "        forms.extend(tmp)\n",
    "        \n",
    "    return forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wor', 'ord', 'wod', 'wrd', 'wr', 'od', 'wo', 'wd', 'rd', 'or']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_deletes('word', edit_distance=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sym_vocab = {}\n",
    "for word in vocab:\n",
    "    sym_vocab[word] = word\n",
    "    forms = n_deletes(word)\n",
    "    for form in forms:\n",
    "        if form not in sym_vocab.keys():\n",
    "            sym_vocab[form] = word\n",
    "        else:\n",
    "            if isinstance(sym_vocab[form], set):\n",
    "                sym_vocab[form].add(word)\n",
    "            else:\n",
    "                sym_vocab[form] = {sym_vocab[form]}\n",
    "                sym_vocab[form].add(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "There are four different comparison pair types:\n",
    "dictionary entry==input entry,\n",
    "delete(dictionary entry,p1)==input entry\n",
    "dictionary entry==delete(input entry,p2)\n",
    "delete(dictionary entry,p1)==delete(input entry,p2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.693787266782073e-06 односторонний\n",
      "3.077514906712829e-05 односторонние\n",
      "3.077514906712829e-05 односторонних\n",
      "1.5387574533564147e-05 односторонним\n"
     ]
    }
   ],
   "source": [
    "for word in sym_vocab['односоронни']:\n",
    "    print(word_freq[word], word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad = open('sents_with_mistakes.txt', encoding='utf8').read().splitlines()\n",
    "good = open('correct_sents.txt', encoding='utf8').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_probable(words, word_freq=word_freq):\n",
    "    freqs = dict()\n",
    "    for word in words:\n",
    "        freqs[word] = word_freq[word]\n",
    "    return max(freqs.items(), key=itemgetter(1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_helper(word, vocab=vocab):\n",
    "    return word in vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'вообще' in vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('вообще', 'вообще')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sym_vocab['вбще'], sym_vocab['вобще']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct(word, sym_vocab=sym_vocab):\n",
    "    if vocab_helper(word):\n",
    "        return word\n",
    "    # we don't expect to reliably spellcheck two- or one-letter words\n",
    "    elif len(word) < 3:\n",
    "        return word\n",
    "    else:\n",
    "        if word in sym_vocab.keys(): \n",
    "            candidates = sym_vocab[word]\n",
    "            if isinstance(candidates, set):\n",
    "                return most_probable(candidates)\n",
    "            else:\n",
    "                return candidates\n",
    "        else:\n",
    "            forms = n_deletes(word)\n",
    "            candidates = []\n",
    "            for form in forms:\n",
    "                if form in sym_vocab.keys():\n",
    "                    cand = sym_vocab[form]\n",
    "                    if isinstance(cand, set):\n",
    "                        candidates.extend(cand)\n",
    "                    else:\n",
    "                        candidates.append(cand)\n",
    "            if candidates:\n",
    "                return most_probable(candidates)\n",
    "            else:\n",
    "                # the word is expected to have errors in it,\n",
    "                # but we don't have a form to replace it with\n",
    "                return f'*{word}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_corrections_list = []\n",
    "\n",
    "def good_corrections(correction, corrections_list=good_corrections_list):\n",
    "    if correction[0] != correction[1]:\n",
    "        corrections_list.append(correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correction_helper(correction, good_prediction_count, bad_prediction_count, error_count, overall, \n",
    "                      keep_good_corrections=True):\n",
    "    append = False\n",
    "    # if we predicted correctly\n",
    "    if correction[0] == correction[2]:\n",
    "        good_prediction_count += 1\n",
    "        if keep_good_corrections:\n",
    "            good_corrections(correction)\n",
    "    # otherwise\n",
    "    else:\n",
    "        bad_prediction_count += 1\n",
    "        append = True\n",
    "    \n",
    "    # actual errors\n",
    "    if correction[0] != correction[1]:\n",
    "        error_count += 1\n",
    "    \n",
    "    # total pairs checked\n",
    "    overall += 1\n",
    "    \n",
    "    return good_prediction_count, bad_prediction_count, error_count, overall, append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = dict()\n",
    "corrections = []\n",
    "good_preds = 0\n",
    "bad_preds = 0\n",
    "errors = 0\n",
    "total = 0\n",
    "\n",
    "for i in range(len(good)):\n",
    "    append = False\n",
    "    pairs = align_words(good[i], bad[i])\n",
    "    for pair in pairs:\n",
    "        left = pair[0]\n",
    "        right = pair[1]\n",
    "        if right in cache.keys():\n",
    "            correction = left, right, cache[right]\n",
    "        else:\n",
    "            corrected_word = correct(right)\n",
    "            correction = left, right, corrected_word\n",
    "            cache[right] = corrected_word\n",
    "\n",
    "\n",
    "        good_preds, bad_preds, errors, total, append = correction_helper(correction,\n",
    "                                                                         good_preds,\n",
    "                                                                         bad_preds,\n",
    "                                                                         errors,\n",
    "                                                                         total)\n",
    "        if append:\n",
    "            corrections.append(correction)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В целом работает не так уж плохо, хотя слова делить всё равно не умеет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('апофеозом', 'опофеозом', 'апофеозом'),\n",
       " ('отсутствие', 'отсуствие', 'отсутствие'),\n",
       " ('основная', 'основая', 'основная'),\n",
       " ('напрасно', 'нарасно', 'напрасно'),\n",
       " ('сегодняшнее', 'сегодяшнее', 'сегодняшнее'),\n",
       " ('потому', 'патаму', 'потому'),\n",
       " ('лучше', 'лчше', 'лучше'),\n",
       " ('компьютерная', 'компютерная', 'компьютерная'),\n",
       " ('что', 'чтото', 'что'),\n",
       " ('участвовать', 'учавствовать', 'участвовать')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_corrections_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('вообще', 'ваще', 'все'), 16),\n",
       " (('кстати', 'кстате', 'сайте'), 15),\n",
       " (('очень', 'оооочень', '*оооочень'), 9),\n",
       " (('это', 'ето', 'место'), 9),\n",
       " (('насчет', 'нащет', 'нет'), 6),\n",
       " (('здесь', 'сдесь', 'есть'), 6),\n",
       " (('можно', 'можна', 'она'), 5),\n",
       " (('девчонки', 'девченки', 'девочки'), 5),\n",
       " (('что', 'што', 'шуток'), 5),\n",
       " (('очень', 'ооооочень', '*ооооочень'), 5)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(corrections).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Для оценки используем будем использовать три метрики:\n",
    "1) процент правильных слов;\n",
    "2) процент исправленных ошибок\n",
    "3) процент ошибочно исправленных правильных слов\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.802691924227318"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_preds/total # верно предсказанные слова (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3029932803909591"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(good_corrections_list)/errors # сколько ошибок исправленно верно (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23579173120457525"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_preds/(total - errors) # ошибочно исправленные слова (3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16321036889332005"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors/total # всего ошибок в  исходном тексте"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```2.``` Добавьте к полученному алгоритму исправления триграммную модель и проверьте, улучшает ли она качество."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tri_model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "for sentence in corpus:\n",
    "    for w1, w2, w3 in nltk.trigrams(sentence, pad_right=True, pad_left=True, left_pad_symbol='<s>', right_pad_symbol='</s>'):\n",
    "        tri_model[(w1, w2)][w3] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
