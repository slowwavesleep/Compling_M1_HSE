{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте алгоритм Symspell. Он похож на алгоритм Норвига, но проще и быстрее. Там к словам в словаре применяется только одна операция - удаление символа (1-n). Чтобы найти исправление из слова тоже удаляются символы и сравниваются с теми, что хранятся в словаре. Оцените качество полученного алгоритма теми же тремя метриками."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@wolfgarbe/1000x-faster-spelling-correction-algorithm-2012-8701fcd87a5f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавьте к полученному алгоритму исправления триграммную модель и проверьте, улучшает ли она качество."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk import sent_tokenize\n",
    "from string import punctuation\n",
    "import gzip\n",
    "import csv\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('lenta-ru-news.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation +=  \"«»—…“”\"\n",
    "punct = set(punctuation )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_words(sent_1, sent_2):\n",
    "    tokens_1 = sent_1.lower().split()\n",
    "    tokens_2 = sent_2.lower().split()\n",
    "    \n",
    "    tokens_1 = [re.sub('(^\\W+|\\W+$)', '', token) for token\n",
    "                in tokens_1 if (set(token) - punct)]\n",
    "    tokens_2 = [re.sub('(^\\W+|\\W+$)', '', token) for token\n",
    "                in tokens_2 if (set(token) - punct)]\n",
    "    \n",
    "    return zip(tokens_1, tokens_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    normalized = [(word.strip(punctuation)) for word \n",
    "                  in text.lower().split()\n",
    "                  if word.isalpha() \n",
    "                  and re.fullmatch(r'[а-я]+', word.strip(punctuation))]\n",
    "    return [word for word in normalized if word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for text in texts[:8000]:\n",
    "    sents = sent_tokenize(text)\n",
    "    norm_sents = [normalize(sent) for sent in sents]\n",
    "    corpus += norm_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "\n",
    "for sent in corpus:\n",
    "    vocab.update(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_deletes_helper(word, min_len=0):\n",
    "    result = set()\n",
    "    # taking word length into account\n",
    "    if len(word) > min_len:\n",
    "        for i in range(len(word)):\n",
    "            result.add(word[:i] + word[i+1:])\n",
    "    else:\n",
    "        result.add(word)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_deletes(word, edit_distance=2, min_len=2):\n",
    "    forms = {word}\n",
    "    while edit_distance > 0:\n",
    "        edit_distance -= 1\n",
    "        new_forms = set()\n",
    "        for form in forms:\n",
    "            new_forms.update(n_deletes_helper(form, min_len))\n",
    "        forms = new_forms\n",
    "        \n",
    "    return forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_deletes('word', edit_distance=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sym_vocab = {}\n",
    "for word in vocab:\n",
    "    forms = n_deletes(word)\n",
    "    for form in forms:\n",
    "        if form not in sym_vocab.keys():\n",
    "            sym_vocab[form] = word\n",
    "        else:\n",
    "            if isinstance(sym_vocab[form], set):\n",
    "                sym_vocab[form].add(word)\n",
    "            else:\n",
    "                sym_vocab[form] = {sym_vocab[form]}\n",
    "                sym_vocab[form].add(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "There are four different comparison pair types:\n",
    "dictionary entry==input entry,\n",
    "delete(dictionary entry,p1)==input entry\n",
    "dictionary entry==delete(input entry,p2)\n",
    "delete(dictionary entry,p1)==delete(input entry,p2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
