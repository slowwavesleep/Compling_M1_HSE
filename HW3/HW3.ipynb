{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```1.``` Реализуйте алгоритм Symspell. Он похож на алгоритм Норвига, но проще и быстрее. Там к словам в словаре применяется только одна операция – удаление символа (1-n). Чтобы найти исправление из слова тоже удаляются символы и сравниваются с теми, что хранятся в словаре. Оцените качество полученного алгоритма теми же тремя метриками.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@wolfgarbe/1000x-faster-spelling-correction-algorithm-2012-8701fcd87a5f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from nltk import sent_tokenize\n",
    "import gzip\n",
    "import csv\n",
    "import pandas as pd\n",
    "import re\n",
    "from operator import itemgetter\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('lenta-ru-news.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_words(sent_1, sent_2):\n",
    "    tokens_1, tokens_2 = normalize(sent_1), normalize(sent_2)\n",
    "    return zip(tokens_1, tokens_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    pat = re.compile(r'[А-Яа-я]+')\n",
    "    normalized = re.findall(pat, text)\n",
    "\n",
    "    return [word.lower() for word in normalized if word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for text in texts[:12000]:\n",
    "    sents = sent_tokenize(text)\n",
    "    norm_sents = [normalize(sent) for sent in sents]\n",
    "    corpus += norm_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "\n",
    "for sent in corpus:\n",
    "    vocab.update(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = Counter()\n",
    "\n",
    "for sent in corpus:\n",
    "    words.update(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = dict(words)\n",
    "\n",
    "for key, value in word_freq.items():\n",
    "    word_freq[key] = value/len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_deletes_helper(word, min_len=0):\n",
    "    result = set()\n",
    "    # taking word length into account\n",
    "    if len(word) > min_len:\n",
    "        for i in range(len(word)):\n",
    "            result.add(word[:i] + word[i+1:])\n",
    "    else:\n",
    "        result.add(word)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_deletes(word, edit_distance=2, min_len=2):\n",
    "    edit_distance -= 1\n",
    "    forms = list(n_deletes_helper(word, min_len))\n",
    "    tmp = set(forms)\n",
    "    while edit_distance > 0:\n",
    "        edit_distance -= 1\n",
    "        new_forms = []\n",
    "        for form in tmp:\n",
    "            new_forms.extend(n_deletes_helper(form, min_len))\n",
    "        tmp = set(new_forms)\n",
    "        forms.extend(tmp)\n",
    "        \n",
    "    return forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ord', 'wrd', 'wor', 'wod', 'wd', 'wo', 'wr', 'od', 'rd', 'or']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_deletes('word', edit_distance=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sym_vocab = {}\n",
    "for word in vocab:\n",
    "    sym_vocab[word] = word\n",
    "    forms = n_deletes(word)\n",
    "    for form in forms:\n",
    "        if form not in sym_vocab.keys():\n",
    "            sym_vocab[form] = word\n",
    "        else:\n",
    "            if isinstance(sym_vocab[form], set):\n",
    "                sym_vocab[form].add(word)\n",
    "            else:\n",
    "                sym_vocab[form] = {sym_vocab[form]}\n",
    "                sym_vocab[form].add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.077514906712829e-05 односторонние\n",
      "1.5387574533564147e-05 односторонним\n",
      "7.693787266782073e-06 односторонний\n",
      "3.077514906712829e-05 односторонних\n"
     ]
    }
   ],
   "source": [
    "for word in sym_vocab['односоронни']:\n",
    "    print(word_freq[word], word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad = open('sents_with_mistakes.txt', encoding='utf8').read().splitlines()\n",
    "good = open('correct_sents.txt', encoding='utf8').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_probable(words, word_freq=word_freq):\n",
    "    freqs = dict()\n",
    "    for word in words:\n",
    "        freqs[word] = word_freq[word]\n",
    "    return max(freqs.items(), key=itemgetter(1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_helper(word, vocab=vocab):\n",
    "    return word in vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'вообще' in vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('вообще', 'вообще')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sym_vocab['вбще'], sym_vocab['вобще']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct(word, sym_vocab=sym_vocab):\n",
    "    if vocab_helper(word):\n",
    "        return word\n",
    "    # we don't expect to reliably spellcheck two- or one-letter words\n",
    "    elif len(word) < 3:\n",
    "        return word\n",
    "    else:\n",
    "        if word in sym_vocab.keys(): \n",
    "            candidates = sym_vocab[word]\n",
    "            if isinstance(candidates, set):\n",
    "                return most_probable(candidates)\n",
    "            else:\n",
    "                return candidates\n",
    "        else:\n",
    "            forms = n_deletes(word)\n",
    "            candidates = []\n",
    "            for form in forms:\n",
    "                if form in sym_vocab.keys():\n",
    "                    cand = sym_vocab[form]\n",
    "                    if isinstance(cand, set):\n",
    "                        candidates.extend(cand)\n",
    "                    else:\n",
    "                        candidates.append(cand)\n",
    "            if candidates:\n",
    "                return most_probable(candidates)\n",
    "            else:\n",
    "                # the word is expected to have errors in it,\n",
    "                # but we don't have a form to replace it with\n",
    "                return f'*{word}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(correct/total)\n",
    "print(mistaken_fixed/total_mistaken)\n",
    "print(correct_broken/total_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(corrections, keep_good=False, g_list=None):\n",
    "    result = defaultdict(lambda: 0)\n",
    "    for (good, bad, corrected) in corrections:\n",
    "        #процент правильных слов\n",
    "        if good == corrected:\n",
    "            result['good_corrections'] += 1\n",
    "\n",
    "        # процент исправленных ошибок\n",
    "        if (good != bad):\n",
    "            result['original_errors'] += 1\n",
    "            if (good == corrected):\n",
    "                result['errors_fixed'] += 1\n",
    "                if keep_good:\n",
    "                    g_list.append((good, bad, corrected))\n",
    "            \n",
    "            \n",
    "        # процент ошибочно исправленных правильных слов\n",
    "        if (good == bad):\n",
    "            result['original good'] += 1\n",
    "            if (good != corrected):\n",
    "                result['bad_corrections'] += 1\n",
    "            \n",
    "    # процент правильных слов\n",
    "    result['good_corrections'] /= len(corrections)\n",
    "    # процент исправленных ошибок\n",
    "    result['errors_fixed'] /= result['original_errors']\n",
    "    # процент ошибочно исправленных правильных слов\n",
    "    result['bad_corrections'] /= result['original good']\n",
    "    \n",
    "    print(f\"Процент правильных слов: {result['good_corrections'] * 100}\")\n",
    "    print(f\"Процент исправленных ошибок: {result['errors_fixed'] * 100}\")\n",
    "    print(f\"процент ошибочно исправленных правильных слов {result['bad_corrections'] * 100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cache = dict()\n",
    "corrections = []\n",
    "\n",
    "for i in range(len(good)):\n",
    "    pairs = align_words(good[i], bad[i])\n",
    "    for pair in pairs:\n",
    "        left = pair[0]\n",
    "        right = pair[1]\n",
    "        if right in cache.keys():\n",
    "            correction = left, right, cache[right]\n",
    "        else:\n",
    "            corrected_word = correct(right)\n",
    "            correction = left, right, corrected_word\n",
    "            cache[right] = corrected_word\n",
    "        corrections.append(correction)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Процент правильных слов: 80.2791625124626\n",
      "Процент исправленных ошибок: 30.36041539401344\n",
      "процент ошибочно исправленных правильных слов 9.984510901942095\n"
     ]
    }
   ],
   "source": [
    "g_list = []\n",
    "metrics(corrections, keep_good=True, g_list=g_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В целом работает не так уж плохо, хотя слова делить всё равно не умеет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('сегодня', 'седня', 'сегодня'), 24),\n",
       " (('вообще', 'вобще', 'вообще'), 19),\n",
       " (('естественно', 'естесственно', 'естественно'), 17),\n",
       " (('хочется', 'хочеться', 'хочется'), 16),\n",
       " (('очень', 'ооочень', 'очень'), 15),\n",
       " (('ничего', 'ничо', 'ничего'), 9),\n",
       " (('как', 'както', 'как'), 8),\n",
       " (('что', 'чтото', 'что'), 7),\n",
       " (('периодически', 'переодически', 'периодически'), 7),\n",
       " (('получается', 'получаеться', 'получается'), 6)]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(g_list).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```2.``` Добавьте к полученному алгоритму исправления триграммную модель и проверьте, улучшает ли она качество."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "tri_model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "for sentence in corpus:\n",
    "    for w1, w2, w3 in nltk.trigrams(sentence, pad_right=True, pad_left=True, left_pad_symbol='<s>', right_pad_symbol='</s>'):\n",
    "        tri_model[(w1, w2)][w3] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bigram in tri_model:\n",
    "    total_count = sum(tri_model[bigram].values())\n",
    "    for target in tri_model[bigram]:\n",
    "        tri_model[bigram][target] /= total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_grams = [['<s>', '<s>'] + normalize(sent) + ['</s>', '</s>'] for sent in good]\n",
    "bad_grams = [['<s>', '<s>'] + normalize(sent) + ['</s>', '</s>'] for sent in bad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_probable_grams(candidates, preceding, model=tri_model, word_freq=word_freq):\n",
    "    model = model[preceding]\n",
    "    possibilities = dict()\n",
    "    for candidate in candidates:\n",
    "        if candidate in model.keys():\n",
    "            #possibilities.append((candidate, model[candidate]))\n",
    "            possibilities[candidate] = model[candidate]\n",
    "\n",
    "    if not possibilities:\n",
    "        possibilities = dict()\n",
    "        for candidate in candidates:\n",
    "            possibilities[candidate] = word_freq[candidate]\n",
    "            \n",
    "    return max(possibilities.items(), key=itemgetter(1))[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'не'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_probable_grams(sym_vocab['не'], preceding=('<s>', '<s>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_grams(word, preceding, sym_vocab=sym_vocab):\n",
    "    if vocab_helper(word):\n",
    "        return word\n",
    "    # having context allows checking all words\n",
    "    #elif len(word) < 3:\n",
    "    #    return word\n",
    "    else:\n",
    "        if word in sym_vocab.keys(): \n",
    "            candidates = sym_vocab[word]\n",
    "            if isinstance(candidates, set):\n",
    "                return most_probable_grams(candidates, preceding)\n",
    "            else:\n",
    "                return candidates\n",
    "        else:\n",
    "            forms = n_deletes(word)\n",
    "            candidates = []\n",
    "            for form in forms:\n",
    "                if form in sym_vocab.keys():\n",
    "                    cand = sym_vocab[form]\n",
    "                    if isinstance(cand, set):\n",
    "                        candidates.extend(cand)\n",
    "                    else:\n",
    "                        candidates.append(cand)\n",
    "            if candidates:\n",
    "                return most_probable(candidates)\n",
    "            else:\n",
    "                # the word is expected to have errors in it,\n",
    "                # but we don't have a form to replace it with\n",
    "                return f'*{word}' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = dict()\n",
    "corrections = []\n",
    "\n",
    "for i in range(len(good_grams)):\n",
    "\n",
    "    append = False\n",
    "    pairs = zip(good_grams[i], bad_grams[i])\n",
    "    trail = []\n",
    "    for pair in pairs:\n",
    "        \n",
    "        if pair[1] == '<s>':\n",
    "            trail.append(pair[1])\n",
    "        \n",
    "        elif pair[1] == '</s>':\n",
    "            continue\n",
    "        \n",
    "        else:\n",
    "            trail.append(pair[1])\n",
    "\n",
    "            trail = trail[-3:]\n",
    "            preceding = tuple(trail[:2])\n",
    "\n",
    "            left = pair[0]\n",
    "            right = pair[1]\n",
    "            \n",
    "            if right in cache.keys():\n",
    "                correction = left, right, cache[right]\n",
    "            else:\n",
    "                corrected_word = correct_grams(right, preceding)\n",
    "                correction = left, right, corrected_word\n",
    "                cache[right] = corrected_word\n",
    "            corrections.append(correction)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Процент правильных слов: 80.24310052804623\n",
      "Процент исправленных ошибок: 30.474452554744524\n",
      "процент ошибочно исправленных правильных слов 10.008340283569641\n"
     ]
    }
   ],
   "source": [
    "g_list = []\n",
    "metrics(corrections, keep_good=True, g_list=g_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('сегодня', 'седня', 'сегодня'), 24),\n",
       " (('вообще', 'вобще', 'вообще'), 19),\n",
       " (('естественно', 'естесственно', 'естественно'), 17),\n",
       " (('хочется', 'хочеться', 'хочется'), 16),\n",
       " (('очень', 'ооочень', 'очень'), 15),\n",
       " (('ничего', 'ничо', 'ничего'), 9),\n",
       " (('как', 'както', 'как'), 8),\n",
       " (('что', 'чтото', 'что'), 7),\n",
       " (('периодически', 'переодически', 'периодически'), 7),\n",
       " (('получается', 'получаеться', 'получается'), 6)]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(g_list).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем склеивать размноженные буквы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_multed(word):\n",
    "    if re.search(r'(.)\\1\\1', word):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deduplicate(word):\n",
    "    letters = re.findall(r'(.)\\1\\1', word)\n",
    "    for letter in letters:\n",
    "        word = re.sub(f'{letter}' + r'{3,}', f'{letter}', word)\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_grams_fancy(word, preceding, sym_vocab=sym_vocab):\n",
    "    if is_dupl(word):\n",
    "        word = deduplicate(word)\n",
    "    if vocab_helper(word):\n",
    "        return word\n",
    "    else:\n",
    "        if word in sym_vocab.keys(): \n",
    "            candidates = sym_vocab[word]\n",
    "            if isinstance(candidates, set):\n",
    "                return most_probable_grams(candidates, preceding)\n",
    "            else:\n",
    "                return candidates\n",
    "        else:\n",
    "            forms = n_deletes(word)\n",
    "            candidates = []\n",
    "            for form in forms:\n",
    "                if form in sym_vocab.keys():\n",
    "                    cand = sym_vocab[form]\n",
    "                    if isinstance(cand, set):\n",
    "                        candidates.extend(cand)\n",
    "                    else:\n",
    "                        candidates.append(cand)\n",
    "            if candidates:\n",
    "                return most_probable(candidates)\n",
    "            else:\n",
    "                # the word is expected to have errors in it,\n",
    "                # but we don't have a form to replace it with\n",
    "                return f'*{word}' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = dict()\n",
    "corrections = []\n",
    "\n",
    "for i in range(len(good_grams)):\n",
    "\n",
    "    append = False\n",
    "    pairs = zip(good_grams[i], bad_grams[i])\n",
    "    trail = []\n",
    "    for pair in pairs:\n",
    "        \n",
    "        if pair[1] == '<s>':\n",
    "            trail.append(pair[1])\n",
    "        \n",
    "        elif pair[1] == '</s>':\n",
    "            continue\n",
    "        \n",
    "        else:\n",
    "            trail.append(pair[1])\n",
    "\n",
    "            trail = trail[-3:]\n",
    "            preceding = tuple(trail[:2])\n",
    "\n",
    "            left = pair[0]\n",
    "            right = pair[1]\n",
    "            \n",
    "            if right in cache.keys():\n",
    "                correction = left, right, cache[right]\n",
    "            else:\n",
    "                corrected_word = correct_grams_fancy(right, preceding)\n",
    "                correction = left, right, corrected_word\n",
    "                cache[right] = corrected_word\n",
    "            corrections.append(correction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наблюдаем некоторый прирост в качестве"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Процент правильных слов: 80.39254757397629\n",
      "Процент исправленных ошибок: 31.386861313868614\n",
      "процент ошибочно исправленных правильных слов 10.008340283569641\n"
     ]
    }
   ],
   "source": [
    "g_list = []\n",
    "metrics(corrections, keep_good=True, g_list=g_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('сегодня', 'седня', 'сегодня'), 24),\n",
       " (('вообще', 'вобще', 'вообще'), 19),\n",
       " (('естественно', 'естесственно', 'естественно'), 17),\n",
       " (('хочется', 'хочеться', 'хочется'), 16),\n",
       " (('очень', 'ооочень', 'очень'), 15),\n",
       " (('очень', 'оооочень', 'очень'), 9),\n",
       " (('ничего', 'ничо', 'ничего'), 9),\n",
       " (('как', 'както', 'как'), 8),\n",
       " (('что', 'чтото', 'что'), 7),\n",
       " (('периодически', 'переодически', 'периодически'), 7)]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(g_list).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
