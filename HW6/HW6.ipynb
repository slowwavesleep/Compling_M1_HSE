{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Домашка основана на этом семинаре - https://github.com/mannefedov/compling_nlp_hse_course/blob/master/notebooks/Embeddings.ipynb\n",
    "\n",
    "Возьмите данные соревнования по определению перефразирования - http://paraphraser.ru/download/get?file_id=1\n",
    "\n",
    "Задание делится на 2 части:\n",
    "\n",
    "1) Векторизуйте тексты с помощью Word2vec модели, обученной самостоятельно, и с помощью модели, взятой с rusvectores (любой). Обучите 2 модели по определению перефразирования на получившихся векторах и проверьте, что работает лучше. \n",
    "Word2Vec нужно обучить на отдельном корпусе (не на парафразах). Можно взять данные из семинара или любые другие. \n",
    "ВАЖНО: Оценивать модели нужно с помощью кросс-валидации! Метрика - f1.\n",
    "\n",
    "2) Преобразуйте тексты в векторы в каждой паре 5 методами  - SVD, NMF, Word2Vec (свой и  русвекторовский), Fastext. У вас должно получиться 5 пар векторов для каждой строчки в датасете. Между векторами каждой пары вычислите косинусную близость (получится 5 чисел для каждой пары). \n",
    "\n",
    "Постройте обучающую выборку из этих близостей . Обучите любую модель (Логрег, Рандом форест или что-то ещё) на этой выборке и оцените качество на кросс-валидации (используйте микросреднюю f1-меру).  Попробуйте улучшить метрику, изменив параметры в методах векторизации.\n",
    "\n",
    "\n",
    "SVD и NMF применяйте к данным напрямую, а w2w и fastext обучите на отдельном корпусе (как в первой части). \n",
    "\n",
    "\n",
    "Оценивание - если вы сделали всё вышеперечисленное - 10 баллов. Каждая ошибка - минус 0.5 балла. \n",
    "\n",
    "Выложите код к себе на гитхаб и вставьте ссылку в поле ниже (в тетрадке должны быть показатели метрик и ваши комментарии)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from lxml import html\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD, NMF, PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_distances, cosine_similarity\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import gensim\n",
    "import numpy as np\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter, defaultdict\n",
    "from string import punctuation\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score\n",
    "from scipy import spatial\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = MorphAnalyzer()\n",
    "punct = punctuation+'«»—…“”*№–'\n",
    "stops = set(stopwords.words('russian'))\n",
    "\n",
    "def normalize(text):\n",
    "    \n",
    "    words = [word.strip(punct) for word in text.lower().split()]\n",
    "    words = [morph.parse(word)[0].normal_form for word in words if word and word not in stops]\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "def tokenize(text):\n",
    "    \n",
    "    words = [word.strip(punct) for word in text.lower().split()]\n",
    "\n",
    "    return ' '.join(words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузка корпуса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('corpus_hum.txt', encoding='utf-8') as file:\n",
    "    data = file.read().splitlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_norm = [normalize(text) for text in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_norm = [text for text in data_norm if text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('norm_corp', 'w', encoding='utf-8') as file:\n",
    "#     for line in data_norm:\n",
    "#         file.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('norm_corp', encoding='utf-8') as file:\n",
    "    data_norm = file.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теги НКРЯ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "mystem = Mystem()\n",
    "\n",
    "mapping = {}\n",
    "\n",
    "# mystem POS tags - > RNC POS tags\n",
    "for line in open('ru-rnc.map.txt'):\n",
    "    ms, ud = line.strip('\\n').split()\n",
    "    mapping[ms] = ud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузка и обучение моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2model = gensim.models.Word2Vec([text.split() for text in data_norm], size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2model.save('w2model.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2model = gensim.models.Word2Vec.load('w2model.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruscorpora_upos_cbow_300_20_2019\n",
    "rv_w2model = gensim.models.KeyedVectors.load_word2vec_format('182/model.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функции, необходимые для создания эмбедингов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tag_word(word, mst_analyzer, pos_map):\n",
    "    \n",
    "    mst_word = mst_analyzer.analyze(word)\n",
    "    try:\n",
    "        mst_word = mst_word[0]['analysis'][0]\n",
    "        lemma = mst_word['lex'].lower().strip()\n",
    "        pos = mst_word['gr'].split(',')[0]\n",
    "        pos = mapping[pos]\n",
    "        \n",
    "    except:\n",
    "        return word\n",
    "    \n",
    "    return f'{lemma}_{pos}' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_from_model(word, pos_map, model, dim=300, pos_tag=False, mst_analyzer=None, cache=None, verbose=False):\n",
    "    if pos_tag:\n",
    "        if not mst_analyzer:\n",
    "            raise ValueError('A morphological analyzer is required with pos_tag set to True')\n",
    "            \n",
    "        if cache is not None:\n",
    "            if word in cache:\n",
    "                word = cache[word]\n",
    "            else:\n",
    "                cache[word] = pos_tag_word(word, mst_analyzer, pos_map)\n",
    "                word = cache[word]\n",
    "        else:\n",
    "            word = pos_tag_word(word, mst_analyzer, pos_map)\n",
    "        \n",
    "    \n",
    "    vector = []\n",
    "    try:\n",
    "        vector = model[word]\n",
    "        \n",
    "    except:\n",
    "        vector = np.zeros(dim)\n",
    "        if verbose:\n",
    "            print(f'Word: {word} not found in the model.')\n",
    "    \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'язык_NOUN'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag_word('язык', mystem, mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cache = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_manvec = vec_from_model('язык', mapping, dim=30, model=w2model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rusvec =  vec_from_model('язык', mapping, dim=300, model=rv_w2model,\n",
    "                              pos_tag=True, mst_analyzer=mystem, cache=word_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'язык': 'язык_NOUN'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((300,), (30,))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_rusvec.shape, test_manvec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# non-zero vectors were returned by the model\n",
    "test_rusvec.any(), test_manvec.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_embedding(text, pos_map, model, dim=300, pos_tag=False, mst_analyzer=None, cache=None, verbose=False):\n",
    "    \n",
    "    text = text.split()\n",
    "    words = Counter(text)\n",
    "    total = len(text)\n",
    "    vector_models = np.zeros((len(text), dim))\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        word_vec = vec_from_model(word, model=model, pos_map=pos_map,\n",
    "                                  dim=dim, pos_tag=pos_tag, mst_analyzer=mst_analyzer,\n",
    "                                  cache=cache, verbose=verbose)\n",
    "        vector_models[i] = word_vec * (words[word]/total)\n",
    "                \n",
    "    if vector_models.any():\n",
    "        vector_model = np.average(vector_models, axis=0)\n",
    "    else:\n",
    "        vector_model = np.zeros((dim))\n",
    "    \n",
    "    return vector_model\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "some_test = text_embedding('Какой-то текст пример.', mapping, model=rv_w2model, dim=300, pos_tag=True, mst_analyzer=mystem, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((300,), True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_test.shape, some_test.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_test = text_embedding('Какой-то текст пример.', mapping, model=w2model, dim=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30,), True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_test.shape, other_test.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузка перифраз"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_xml = html.fromstring(open('paraphrases.xml', 'rb').read())\n",
    "texts_1 = []\n",
    "texts_2 = []\n",
    "classes = []\n",
    "\n",
    "for p in corpus_xml.xpath('//paraphrase'):\n",
    "    texts_1.append(p.xpath('./value[@name=\"text_1\"]/text()')[0])\n",
    "    texts_2.append(p.xpath('./value[@name=\"text_2\"]/text()')[0])\n",
    "    classes.append(p.xpath('./value[@name=\"class\"]/text()')[0])\n",
    "    \n",
    "para = pd.DataFrame({'text_1':texts_1, 'text_2':texts_2, 'label':classes})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "para['text_1_norm'] = para['text_1'].apply(normalize)\n",
    "para['text_2_norm'] = para['text_2'].apply(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text_1 = [text_embedding(text, mapping, model = rv_w2model,\n",
    "            dim=300, pos_tag=True, mst_analyzer=mystem, cache=word_cache) for text\n",
    "           in para['text_1_norm']]\n",
    "X_text_2 = [text_embedding(text, mapping, model = rv_w2model,\n",
    "            dim=300, pos_tag=True, mst_analyzer=mystem, cache=word_cache) for text\n",
    "           in para['text_2_norm']]\n",
    "\n",
    "X_text = np.concatenate([X_text_1, X_text_2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7227, 600)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text_1_cust = [text_embedding(text, mapping, model = w2model,\n",
    "            dim=30) for text\n",
    "           in para['text_1_norm']]\n",
    "X_text_2_cust = [text_embedding(text, mapping, model = w2model,\n",
    "            dim=30) for text\n",
    "           in para['text_2_norm']]\n",
    "\n",
    "X_text_cust = np.concatenate([X_text_1_cust, X_text_2_cust], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7227, 60)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_text_cust.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = para.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(C=1000, class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rusvectores model results:\n",
      "CV score values: [0.39737388 0.40774015 0.44705882 0.35249307 0.36565097]\n",
      "Average F-score: 0.39406337937526237\n"
     ]
    }
   ],
   "source": [
    "cv_scores_1 = cross_val_score(clf, X_text, y, scoring='f1_micro', cv=5)\n",
    "print('Rusvectores model results:')\n",
    "print(f'CV score values: {cv_scores_1}')\n",
    "print(f'Average F-score: {np.mean(cv_scores_1)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom w2v model results:\n",
      "CV score values: [0.42225294 0.44298549 0.47058824 0.39819945 0.3767313 ]\n",
      "Average F-score: 0.42215148150854953\n"
     ]
    }
   ],
   "source": [
    "cv_scores_2 = cross_val_score(clf, X_text_cust, y, scoring='f1_micro', cv=5)\n",
    "print('Custom w2v model results:')\n",
    "print(f'CV score values: {cv_scores_2}')\n",
    "print(f'Average F-score: {np.mean(cv_scores_2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что готовая модель не настолько уж сильно лучше. Значительное преимущество в объеме и количестве признаков дает непропорциональный прирост в качестве. Предположу, что если хорошо подобрать тексты, то своя модель будет работать даже лучше готовой, имея при этом меньшее количество фич."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# костыльное решение, но я пока разобрался как это нормально сделать в пандасе\n",
    "w2v_sim = []\n",
    "for x, y in zip(X_text_1, X_text_2):\n",
    "    if x.any() and y.any():\n",
    "        w2v_sim.append(1 - spatial.distance.cosine(x, y))\n",
    "    else:\n",
    "        w2v_sim.append(0)\n",
    "w2v_sim = np.array(w2v_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "para['w2v_sim'] = w2v_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_sim_cus = []\n",
    "for x, y in zip(X_text_1_cust, X_text_2_cust):\n",
    "    if x.any() and y.any():\n",
    "        w2v_sim_cus.append(1 - spatial.distance.cosine(x, y))\n",
    "    else:\n",
    "        w2v_sim_cus.append(0)\n",
    "w2v_sim_cus = np.array(w2v_sim_cus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "para ['w2v_sim_cus'] = w2v_sim_cus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rusvectores model results:\n",
      "CV score values: [0.57014513 0.58396683 0.59238754 0.45844875 0.49792244]\n",
      "Average F-score: 0.5405741380317778\n"
     ]
    }
   ],
   "source": [
    "cv_sim = cross_val_score(clf, w2v_sim.reshape(-1, 1), para.label.values, scoring='f1_micro', cv=5)\n",
    "print('Rusvectores model results:')\n",
    "print(f'CV score values: {cv_sim}')\n",
    "print(f'Average F-score: {np.mean(cv_sim)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom w2v model results:\n",
      "CV score values: [0.53489979 0.54872149 0.55847751 0.44736842 0.45498615]\n",
      "Average F-score: 0.5088906729411489\n"
     ]
    }
   ],
   "source": [
    "cv_cus = cross_val_score(clf, w2v_sim_cus.reshape(-1, 1), para.label.values, scoring='f1_micro', cv=5)\n",
    "print('Custom w2v model results:')\n",
    "print(f'CV score values: {cv_cus}')\n",
    "print(f'Average F-score: {np.mean(cv_cus)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model = gensim.models.KeyedVectors.load(\"araneum_none_fasttextcbow_300_5_2018.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_1 = ft_model[para['text_1']]\n",
    "ft_2 = ft_model[para['text_2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_sim = []\n",
    "for x, y in zip(ft_1, ft_2):\n",
    "    if x.any() and y.any():\n",
    "        ft_sim.append(1 - spatial.distance.cosine(x, y))\n",
    "    else:\n",
    "        ft_sim.append(0)\n",
    "ft_sim = np.array(ft_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fasttext model results:\n",
      "CV score values: [0.55079475 0.57567381 0.5799308  0.49445983 0.50761773]\n",
      "Average F-score: 0.5416953827613927\n"
     ]
    }
   ],
   "source": [
    "cv_ft = cross_val_score(clf, ft_sim.reshape(-1, 1), para.label.values, scoring='f1_micro', cv=5)\n",
    "print('Fasttext model results:')\n",
    "print(f'CV score values: {cv_ft}')\n",
    "print(f'Average F-score: {np.mean(cv_ft)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=0.4, max_features=1000,\n",
       "                min_df=3, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(min_df=3, max_df=0.4, max_features=1000)\n",
    "tfidf.fit(pd.concat([para['text_1_norm'], para['text_2_norm']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=200, n_iter=5,\n",
       "             random_state=None, tol=0.0)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd = TruncatedSVD(200)\n",
    "\n",
    "svd.fit(tfidf.transform(data_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text_1_svd = svd.fit_transform(tfidf.transform(para['text_1_norm']))\n",
    "X_text_2_svd = svd.fit_transform(tfidf.transform(para['text_2_norm']))\n",
    "\n",
    "X_text_svd = np.concatenate([X_text_1_svd, X_text_2_svd], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_sim = []\n",
    "for x, y in zip(X_text_1_svd, X_text_2_svd):\n",
    "    if x.any() and y.any():\n",
    "        svd_sim.append(1 - spatial.distance.cosine(x, y))\n",
    "    else:\n",
    "        svd_sim.append(0)\n",
    "svd_sim = np.array(svd_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD results:\n",
      "CV score values: [0.40912232 0.40635798 0.40622837 0.41620499 0.3566482 ]\n",
      "Average F-score: 0.3989123726750783\n"
     ]
    }
   ],
   "source": [
    "cv_svd = cross_val_score(clf, svd_sim.reshape(-1, 1), para.label.values, scoring='f1_micro', cv=5)\n",
    "print('SVD results:')\n",
    "print(f'CV score values: {cv_svd}')\n",
    "print(f'Average F-score: {np.mean(cv_svd)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=200,\n",
       "    n_components=30, random_state=None, shuffle=False, solver='cd', tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf = NMF(30)\n",
    "nmf.fit(tfidf.transform(data_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text_1_nmf = nmf.transform(tfidf.transform(para['text_1_norm']))\n",
    "X_text_2_nmf = nmf.transform(tfidf.transform(para['text_2_norm']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_sim = []\n",
    "for x, y in zip(X_text_1_nmf, X_text_2_nmf):\n",
    "    if x.any() and y.any():\n",
    "        nmf_sim.append(1 - spatial.distance.cosine(x, y))\n",
    "    else:\n",
    "        nmf_sim.append(0)\n",
    "nmf_sim = np.array(nmf_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMF results:\n",
      "CV score values: [0.49274361 0.49274361 0.50865052 0.43559557 0.42174515]\n",
      "Average F-score: 0.47029569083603695\n"
     ]
    }
   ],
   "source": [
    "cv_nmf = cross_val_score(clf, nmf_sim.reshape(-1, 1), para.label.values, scoring='f1_micro', cv=5)\n",
    "print('NMF results:')\n",
    "print(f'CV score values: {cv_nmf}')\n",
    "print(f'Average F-score: {np.mean(cv_nmf)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Резюмирую, что самой удобной моделью из рассмотренных можно считать Fasttext, т.к. она не требует нормализации текстов и при этом даёт результат *лучше*, чем Word2Vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
