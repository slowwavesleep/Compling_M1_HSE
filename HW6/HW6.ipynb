{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Домашка основана на этом семинаре - https://github.com/mannefedov/compling_nlp_hse_course/blob/master/notebooks/Embeddings.ipynb\n",
    "\n",
    "Возьмите данные соревнования по определению перефразирования - http://paraphraser.ru/download/get?file_id=1\n",
    "\n",
    "Задание делится на 2 части:\n",
    "\n",
    "1) Векторизуйте тексты с помощью Word2vec модели, обученной самостоятельно, и с помощью модели, взятой с rusvectores (любой). Обучите 2 модели по определению перефразирования на получившихся векторах и проверьте, что работает лучше. \n",
    "Word2Vec нужно обучить на отдельном корпусе (не на парафразах). Можно взять данные из семинара или любые другие. \n",
    "ВАЖНО: Оценивать модели нужно с помощью кросс-валидации! Метрика - f1.\n",
    "\n",
    "2) Преобразуйте тексты в векторы в каждой паре 5 методами  - SVD, NMF, Word2Vec (свой и  русвекторовский), Fastext. У вас должно получиться 5 пар векторов для каждой строчки в датасете. Между векторами каждой пары вычислите косинусную близость (получится 5 чисел для каждой пары). \n",
    "\n",
    "Постройте обучающую выборку из этих близостей . Обучите любую модель (Логрег, Рандом форест или что-то ещё) на этой выборке и оцените качество на кросс-валидации (используйте микросреднюю f1-меру).  Попробуйте улучшить метрику, изменив параметры в методах векторизации.\n",
    "\n",
    "\n",
    "SVD и NMF применяйте к данным напрямую, а w2w и fastext обучите на отдельном корпусе (как в первой части). \n",
    "\n",
    "\n",
    "Оценивание - если вы сделали всё вышеперечисленное - 10 баллов. Каждая ошибка - минус 0.5 балла. \n",
    "\n",
    "Выложите код к себе на гитхаб и вставьте ссылку в поле ниже (в тетрадке должны быть показатели метрик и ваши комментарии)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from lxml import html\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD, NMF, PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_distances, cosine_similarity\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import gensim\n",
    "import numpy as np\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter, defaultdict\n",
    "from string import punctuation\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score\n",
    "from scipy import spatial\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = MorphAnalyzer()\n",
    "punct = punctuation+'«»—…“”*№–'\n",
    "stops = set(stopwords.words('russian'))\n",
    "\n",
    "def normalize(text):\n",
    "    \n",
    "    words = [word.strip(punct) for word in text.lower().split()]\n",
    "    words = [morph.parse(word)[0].normal_form for word in words if word and word not in stops]\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "def tokenize(text):\n",
    "    \n",
    "    words = [word.strip(punct) for word in text.lower().split()]\n",
    "\n",
    "    return ' '.join(words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузка корпуса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('corpus_hum.txt', encoding='utf-8') as file:\n",
    "    data = file.read().splitlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_norm = [normalize(text) for text in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_norm = [text for text in data_norm if text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('norm_corp', 'w', encoding='utf-8') as file:\n",
    "#     for line in data_norm:\n",
    "#         file.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('norm_corp', encoding='utf-8') as file:\n",
    "    data_norm = file.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теги НКРЯ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "mystem = Mystem()\n",
    "\n",
    "mapping = {}\n",
    "\n",
    "# mystem POS tags - > RNC POS tags\n",
    "for line in open('ru-rnc.map.txt'):\n",
    "    ms, ud = line.strip('\\n').split()\n",
    "    mapping[ms] = ud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузка и обучение моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2model = gensim.models.Word2Vec([text.split() for text in data_norm], size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2model.save('w2model.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2model = gensim.models.Word2Vec.load('w2model.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruscorpora_upos_cbow_300_20_2019\n",
    "rv_w2model = gensim.models.KeyedVectors.load_word2vec_format('182/model.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функции, необходимые для создания эмбедингов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#готовые модели w2v требуют POS тегов, поэтому требуется их отдельно проставлять\n",
    "\n",
    "def pos_tag_word(word, mst_analyzer, pos_map):\n",
    "    \n",
    "    mst_word = mst_analyzer.analyze(word)\n",
    "    try:\n",
    "        mst_word = mst_word[0]['analysis'][0]\n",
    "        lemma = mst_word['lex'].lower().strip()\n",
    "        pos = mst_word['gr'].split(',')[0]\n",
    "        pos = mapping[pos]\n",
    "        \n",
    "    except:\n",
    "        return word\n",
    "    \n",
    "    return f'{lemma}_{pos}' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_from_model(word, pos_map, model, dim=300, pos_tag=False, mst_analyzer=None, cache=None, verbose=False):\n",
    "    if pos_tag:\n",
    "        if not mst_analyzer:\n",
    "            raise ValueError('A morphological analyzer is required with pos_tag set to True')\n",
    "        \n",
    "        # кэш для ускорения скорости работы функции\n",
    "        if cache is not None:\n",
    "            if word in cache:\n",
    "                word = cache[word]\n",
    "            else:\n",
    "                cache[word] = pos_tag_word(word, mst_analyzer, pos_map)\n",
    "                word = cache[word]\n",
    "        else:\n",
    "            word = pos_tag_word(word, mst_analyzer, pos_map)\n",
    "        \n",
    "    \n",
    "    vector = []\n",
    "    try:\n",
    "        vector = model[word]\n",
    "        \n",
    "    except:\n",
    "        # на случай, если модели не удалось выдать данное слово\n",
    "        vector = np.zeros(dim)\n",
    "        if verbose:\n",
    "            print(f'Word: {word} not found in the model.')\n",
    "    \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'язык_NOUN'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag_word('язык', mystem, mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cache = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_manvec = vec_from_model('язык', mapping, dim=30, model=w2model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rusvec =  vec_from_model('язык', mapping, dim=300, model=rv_w2model,\n",
    "                              pos_tag=True, mst_analyzer=mystem, cache=word_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'язык': 'язык_NOUN'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((300,), (30,))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_rusvec.shape, test_manvec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# non-zero vectors were returned by the model\n",
    "test_rusvec.any(), test_manvec.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_embedding(text, pos_map, model, dim=300, pos_tag=False, mst_analyzer=None, cache=None, verbose=False):\n",
    "    \n",
    "    text = text.split()\n",
    "    words = Counter(text)\n",
    "    total = len(text)\n",
    "    vector_models = np.zeros((len(text), dim))\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        word_vec = vec_from_model(word, model=model, pos_map=pos_map,\n",
    "                                  dim=dim, pos_tag=pos_tag, mst_analyzer=mst_analyzer,\n",
    "                                  cache=cache, verbose=verbose)\n",
    "        vector_models[i] = word_vec * (words[word]/total)\n",
    "                \n",
    "    if vector_models.any():\n",
    "        vector_model = np.average(vector_models, axis=0)\n",
    "    else:\n",
    "        vector_model = np.zeros((dim))\n",
    "    \n",
    "    return vector_model\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверяю, что все работает как надо"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "some_test = text_embedding('Какой-то текст пример.', mapping, model=rv_w2model, dim=300, pos_tag=True, mst_analyzer=mystem, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((300,), True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_test.shape, some_test.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_test = text_embedding('Какой-то текст пример.', mapping, model=w2model, dim=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30,), True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_test.shape, other_test.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузка перифраз"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_xml = html.fromstring(open('paraphrases.xml', 'rb').read())\n",
    "texts_1 = []\n",
    "texts_2 = []\n",
    "classes = []\n",
    "\n",
    "for p in corpus_xml.xpath('//paraphrase'):\n",
    "    texts_1.append(p.xpath('./value[@name=\"text_1\"]/text()')[0])\n",
    "    texts_2.append(p.xpath('./value[@name=\"text_2\"]/text()')[0])\n",
    "    classes.append(p.xpath('./value[@name=\"class\"]/text()')[0])\n",
    "    \n",
    "para = pd.DataFrame({'text_1':texts_1, 'text_2':texts_2, 'label':classes})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "para['text_1_norm'] = para['text_1'].apply(normalize)\n",
    "para['text_2_norm'] = para['text_2'].apply(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text_1 = [text_embedding(text, mapping, model = rv_w2model,\n",
    "            dim=300, pos_tag=True, mst_analyzer=mystem, cache=word_cache) for text\n",
    "           in para['text_1_norm']]\n",
    "X_text_2 = [text_embedding(text, mapping, model = rv_w2model,\n",
    "            dim=300, pos_tag=True, mst_analyzer=mystem, cache=word_cache) for text\n",
    "           in para['text_2_norm']]\n",
    "\n",
    "X_text = np.concatenate([X_text_1, X_text_2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7227, 600)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text_1_cust = [text_embedding(text, mapping, model = w2model,\n",
    "            dim=30) for text\n",
    "           in para['text_1_norm']]\n",
    "X_text_2_cust = [text_embedding(text, mapping, model = w2model,\n",
    "            dim=30) for text\n",
    "           in para['text_2_norm']]\n",
    "\n",
    "X_text_cust = np.concatenate([X_text_1_cust, X_text_2_cust], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7227, 60)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_text_cust.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = para.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(C=1000, class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rusvectores model results:\n",
      "CV score values: [0.39737388 0.40774015 0.44705882 0.35249307 0.36565097]\n",
      "Average F-score: 0.39406337937526237\n"
     ]
    }
   ],
   "source": [
    "cv_scores_1 = cross_val_score(clf, X_text, y, scoring='f1_micro', cv=5)\n",
    "print('Rusvectores model results:')\n",
    "print(f'CV score values: {cv_scores_1}')\n",
    "print(f'Average F-score: {np.mean(cv_scores_1)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom w2v model results:\n",
      "CV score values: [0.42225294 0.44298549 0.47058824 0.39819945 0.3767313 ]\n",
      "Average F-score: 0.42215148150854953\n"
     ]
    }
   ],
   "source": [
    "cv_scores_2 = cross_val_score(clf, X_text_cust, y, scoring='f1_micro', cv=5)\n",
    "print('Custom w2v model results:')\n",
    "print(f'CV score values: {cv_scores_2}')\n",
    "print(f'Average F-score: {np.mean(cv_scores_2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что готовая модель не настолько уж сильно лучше. Значительное преимущество в объеме и количестве признаков дает непропорциональный прирост в качестве. Предположу, что если хорошо подобрать тексты, то своя модель будет работать даже лучше готовой, имея при этом меньшее количество фич."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# костыльное решение, но я пока разобрался как это нормально сделать в пандасе\n",
    "w2v_sim = []\n",
    "for x, y in zip(X_text_1, X_text_2):\n",
    "    if x.any() and y.any():\n",
    "        w2v_sim.append(1 - spatial.distance.cosine(x, y))\n",
    "    else:\n",
    "        w2v_sim.append(0)\n",
    "w2v_sim = np.array(w2v_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "para['w2v_sim'] = w2v_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# правильно было бы положить это в функцию, но пока не стал, чтобы не путаться\n",
    "w2v_sim_cus = []\n",
    "for x, y in zip(X_text_1_cust, X_text_2_cust):\n",
    "    if x.any() and y.any():\n",
    "        w2v_sim_cus.append(1 - spatial.distance.cosine(x, y))\n",
    "    else:\n",
    "        w2v_sim_cus.append(0)\n",
    "w2v_sim_cus = np.array(w2v_sim_cus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "para ['w2v_sim_cus'] = w2v_sim_cus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rusvectores model results:\n",
      "CV score values: [0.57014513 0.58396683 0.59238754 0.45844875 0.49792244]\n",
      "Average F-score: 0.5405741380317778\n"
     ]
    }
   ],
   "source": [
    "cv_sim = cross_val_score(clf, w2v_sim.reshape(-1, 1), para.label.values, scoring='f1_micro', cv=5)\n",
    "print('Rusvectores model results:')\n",
    "print(f'CV score values: {cv_sim}')\n",
    "print(f'Average F-score: {np.mean(cv_sim)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кажется, что самостоятельно обученная модель не намного хуже"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom w2v model results:\n",
      "CV score values: [0.53489979 0.54872149 0.55847751 0.44736842 0.45498615]\n",
      "Average F-score: 0.5088906729411489\n"
     ]
    }
   ],
   "source": [
    "cv_cus = cross_val_score(clf, w2v_sim_cus.reshape(-1, 1), para.label.values, scoring='f1_micro', cv=5)\n",
    "print('Custom w2v model results:')\n",
    "print(f'CV score values: {cv_cus}')\n",
    "print(f'Average F-score: {np.mean(cv_cus)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model = gensim.models.KeyedVectors.load(\"araneum_none_fasttextcbow_300_5_2018.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_1 = ft_model[para['text_1']]\n",
    "ft_2 = ft_model[para['text_2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_sim = []\n",
    "for x, y in zip(ft_1, ft_2):\n",
    "    if x.any() and y.any():\n",
    "        ft_sim.append(1 - spatial.distance.cosine(x, y))\n",
    "    else:\n",
    "        ft_sim.append(0)\n",
    "ft_sim = np.array(ft_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "para['ft_sim'] = ft_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fasttext model results:\n",
      "CV score values: [0.55079475 0.57567381 0.5799308  0.49445983 0.50761773]\n",
      "Average F-score: 0.5416953827613927\n"
     ]
    }
   ],
   "source": [
    "cv_ft = cross_val_score(clf, ft_sim.reshape(-1, 1), para.label.values, scoring='f1_micro', cv=5)\n",
    "print('Fasttext model results:')\n",
    "print(f'CV score values: {cv_ft}')\n",
    "print(f'Average F-score: {np.mean(cv_ft)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Любопытно, что использование нормализованных текстов все же повышает качество."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_1_norm = ft_model[para['text_1_norm']]\n",
    "ft_2_norm = ft_model[para['text_2_norm']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_sim_norm = []\n",
    "for x, y in zip(ft_1_norm, ft_2_norm):\n",
    "    if x.any() and y.any():\n",
    "        ft_sim_norm.append(1 - spatial.distance.cosine(x, y))\n",
    "    else:\n",
    "        ft_sim_norm.append(0)\n",
    "ft_sim_norm = np.array(ft_sim_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "para['ft_sim_norm'] = ft_sim_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fasttext model results:\n",
      "CV score values: [0.56323428 0.57774706 0.60069204 0.51246537 0.49376731]\n",
      "Average F-score: 0.5495812138416014\n"
     ]
    }
   ],
   "source": [
    "cv_ft_norm = cross_val_score(clf, ft_sim_norm.reshape(-1, 1), para.label.values, scoring='f1_micro', cv=5)\n",
    "print('Fasttext model results:')\n",
    "print(f'CV score values: {cv_ft_norm}')\n",
    "print(f'Average F-score: {np.mean(cv_ft_norm)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пробовал добавлять ngram_range, но ничего не дало. Остальные параметры подкрутил до максимального качества."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=0.7, max_features=1200,\n",
       "                min_df=3, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(min_df=3, max_df=0.7, max_features=1200)\n",
    "tfidf.fit(pd.concat([para['text_1_norm'], para['text_2_norm']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Увеличение количества компонентов и количества итераций дает небольшой приросто в качестве. Свыше 500 прироста нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=500, n_iter=20,\n",
       "             random_state=42, tol=0.0)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd = TruncatedSVD(500, n_iter=20, random_state=42)\n",
    "\n",
    "svd.fit(tfidf.transform(pd.concat([para['text_1_norm'], para['text_2_norm']])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text_1_svd = svd.transform(tfidf.transform(para['text_1_norm']))\n",
    "X_text_2_svd = svd.transform(tfidf.transform(para['text_2_norm']))\n",
    "\n",
    "X_text_svd = np.concatenate([X_text_1_svd, X_text_2_svd], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_sim = []\n",
    "for x, y in zip(X_text_1_svd, X_text_2_svd):\n",
    "    if x.any() and y.any():\n",
    "        svd_sim.append(1 - spatial.distance.cosine(x, y))\n",
    "    else:\n",
    "        svd_sim.append(0)\n",
    "svd_sim = np.array(svd_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "para['svd_sim'] = svd_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD results:\n",
      "CV score values: [0.55148583 0.57774706 0.57716263 0.44736842 0.44598338]\n",
      "Average F-score: 0.5199494651915932\n"
     ]
    }
   ],
   "source": [
    "cv_svd = cross_val_score(clf, svd_sim.reshape(-1, 1), para.label.values, scoring='f1_micro', cv=5)\n",
    "print('SVD results:')\n",
    "print(f'CV score values: {cv_svd}')\n",
    "print(f'Average F-score: {np.mean(cv_svd)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Использование параметра init повышает скорость обучения и качество. Другие параметры также несколько повысили качество."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.2, beta_loss='frobenius', init='nndsvd', l1_ratio=0.0, max_iter=200,\n",
       "    n_components=50, random_state=42, shuffle=False, solver='mu', tol=0.1,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf = NMF(50, init='nndsvd', random_state=42, alpha=0.2, tol=1e-1, solver='mu')\n",
    "nmf.fit(tfidf.transform(pd.concat([para['text_1_norm'], para['text_2_norm']])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text_1_nmf = nmf.transform(tfidf.transform(para['text_1_norm']))\n",
    "X_text_2_nmf = nmf.transform(tfidf.transform(para['text_2_norm']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_sim = []\n",
    "for x, y in zip(X_text_1_nmf, X_text_2_nmf):\n",
    "    if x.any() and y.any():\n",
    "        nmf_sim.append(1 - spatial.distance.cosine(x, y))\n",
    "    else:\n",
    "        nmf_sim.append(0)\n",
    "nmf_sim = np.array(nmf_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "para['nmf_sim'] = nmf_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMF results:\n",
      "CV score values: [0.51485833 0.54941258 0.53979239 0.41897507 0.38504155]\n",
      "Average F-score: 0.48161598267264444\n"
     ]
    }
   ],
   "source": [
    "cv_nmf = cross_val_score(clf, nmf_sim.reshape(-1, 1), para.label.values, scoring='f1_micro', cv=5)\n",
    "print('NMF results:')\n",
    "print(f'CV score values: {cv_nmf}')\n",
    "print(f'Average F-score: {np.mean(cv_nmf)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Резюмирую, что самой удобной моделью из рассмотренных можно считать Fasttext, т.к. она не требует нормализации текстов (хотя она может быть полезна) и при этом даёт результат *лучше*, чем Word2Vec. В целом её проще использовать"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель, обученная на всех векторных близостях одновременно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = para[['w2v_sim', 'w2v_sim_cus', 'ft_sim', 'ft_sim_norm', 'svd_sim', 'nmf_sim']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All models result:\n",
      "CV score values: [0.58811334 0.6081548  0.61384083 0.51800554 0.51246537]\n",
      "Average F-score: 0.5681159771117184\n"
     ]
    }
   ],
   "source": [
    "all_models = cross_val_score(clf, feats, para.label.values, scoring='f1_micro', cv=5)\n",
    "print('All models result:')\n",
    "print(f'CV score values: {all_models}')\n",
    "print(f'Average F-score: {np.mean(all_models)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
